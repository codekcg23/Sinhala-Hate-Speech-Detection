{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogQ-wIb5KhYH"
      },
      "source": [
        "### Experiment :\n",
        "### Objectives\n",
        "\n",
        "1. Train on embeddings and withou embeddinsgs\n",
        "4. Error analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikmcc1WMKgNK"
      },
      "source": [
        "### Observations\n",
        "\n",
        "1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF8BkDzFKg9f"
      },
      "source": [
        "### Solutions and further investigation\n",
        "\n",
        "1. \n",
        "2. \n",
        "3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2xtbDAWLGVd"
      },
      "source": [
        "### Load Modeules and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vT-p5EVbKThs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random as rn\n",
        "np.random.seed(0)\n",
        "rn.seed(0)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "# import helper function script\n",
        "import sys\n",
        "sys.path.insert(1,'G:\\\\Github\\\\Sinhala-Hate-Speech-Detection')\n",
        "import utills\n",
        "import TextClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "gru_len = 128\n",
        "Routings = 5\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "dropout_p = 0.3\n",
        "rate_drop_dense = 0.3\n",
        "import tensorflow as tf\n",
        "from keras.engine import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from  tensorflow.keras.layers import (\n",
        "\n",
        "    Activation,\n",
        "    Bidirectional,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    GRU,\n",
        "    Input,\n",
        "    SpatialDropout1D,\n",
        ")\n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
        "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return x / scale\n",
        "    \n",
        "class Capsule(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
        "                 activation='default', **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_size = kernel_size\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'default':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = Activation(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Capsule, self).build(input_shape)\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(1, input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     # shape=self.kernel_size,\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(input_num_capsule,\n",
        "                                            input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        if self.share_weights:\n",
        "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
        "        else:\n",
        "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(u_vecs)[0]\n",
        "        input_num_capsule = K.shape(u_vecs)[1]\n",
        "        print(\"input_num_capsule =\",input_num_capsule )\n",
        "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
        "                                            self.num_capsule, self.dim_capsule))\n",
        "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        print(\"u-hat \",u_hat_vecs)\n",
        "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
        "        print(\"b = \",b.shape)\n",
        "        for i in range(self.routings):\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
        "            c = K.softmax(b)\n",
        "            c = K.permute_dimensions(c, (0, 2, 1))\n",
        "            b = K.permute_dimensions(b, (0, 2, 1))\n",
        "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
        "            #outputs = self.activation(tf.matmul(c,u_hat_vecs))\n",
        "            print(\"u-hat \",u_hat_vecs)\n",
        "            print(\"outputs = \",outputs.shape)\n",
        "            if i < self.routings - 1:\n",
        "                print(\"b = \",b.shape)\n",
        "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
        "               #b = tf.matmul(outputs,u_hat_vecs,transpose_b=True)\n",
        "                #b = tf.matmul(outputs,u_hat_vecs)\n",
        "\n",
        "\n",
        "        return outputs #tf.squeeze(outputs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, 1, 30)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# uncderstanding matmul vs batch_dot\n",
        "import tensorflow as tf\n",
        "from keras.engine import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "x_batch = K.ones(shape=(32, 20, 1))\n",
        "y_batch = K.ones(shape=(32, 30, 20))\n",
        "xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
        "K.int_shape(xy_batch_dot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(TC):\n",
        "    from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
        "    inp = Input( shape=(TC.MAX_SEQ_LEN,))\n",
        "    if(TC.EMBEDDING == None):\n",
        "        embedding_layer = Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
        "                        input_dim=TC.LEN_VOCAB, \n",
        "                        input_length=TC.MAX_SEQ_LEN,\n",
        "                        #weights=[TC.emb_matrix], # Additionally we give the Wi\n",
        "                        trainable=TC.trainable)(inp)\n",
        "    else:\n",
        "        embedding_layer = Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
        "                            input_dim=TC.LEN_VOCAB, \n",
        "                            input_length=TC.MAX_SEQ_LEN,\n",
        "                            weights=[TC.emb_matrix], # Additionally we give the Wi\n",
        "                            trainable=TC.trainable)(inp)\n",
        "    embedding_layer = SpatialDropout1D(rate_drop_dense)(embedding_layer)\n",
        "    x = Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
        "        embedding_layer)\n",
        "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True)(x)\n",
        "    \n",
        "\n",
        "    capsule = Flatten()(capsule)\n",
        "    capsule = Dropout(dropout_p)(capsule)\n",
        "    output = Dense(1, activation=\"sigmoid\")(capsule)\n",
        "    model = Model(inputs=inp, outputs=output)\n",
        "    optimizer_adam = Adam(learning_rate=TC.lr)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optimizer_adam,\n",
        "                    metrics=['acc'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_routing = 4\n",
        "num_capsule = 4\n",
        "dim_capsule = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    input1_pre = Input(shape=(maxlen,))\n",
        "    embed_layer1_pre = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=False)(input1_pre)\n",
        "    embed_layer1_pre = SpatialDropout1D(0.4)(embed_layer1_pre)\n",
        "\n",
        "    x_pre = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_layer1_pre)\n",
        "    capsule_pre1 = PrimaryCap(x_pre,16,1,1)\n",
        "    capsule_pre2 = PrimaryCap(x_pre,16,1,3)\n",
        "    capsule_pre3 = PrimaryCap(x_pre,16,1,5)\n",
        "    toxiccaps_pre1 = CapsuleLayer(num_capsule=6, dim_capsule=16, routings=7, name='toxiccapspre1')(capsule_pre1)\n",
        "    toxiccaps_pre2 = CapsuleLayer(num_capsule=6, dim_capsule=16, routings=7, name='toxiccapspre2')(capsule_pre2)\n",
        "    toxiccaps_pre3 = CapsuleLayer(num_capsule=6, dim_capsule=16, routings=7, name='toxiccapspre3')(capsule_pre3)\n",
        "#     capsule_pre = Capsule(num_capsule=10, dim_capsule=16, routings=5,share_weights=True)(x_pre)\n",
        "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
        "    toxiccaps_pre = concatenate([toxiccaps_pre1,toxiccaps_pre2,toxiccaps_pre3])\n",
        "    capsule_pre = GlobalMaxPooling1D()(toxiccaps_pre)\n",
        "    capsule_pre = Dropout(0.25)(capsule_pre)\n",
        "\n",
        "    input1_post = Input(shape=(maxlen,))\n",
        "    embed_layer1_post = Embedding(max_features,\n",
        "                            embed_size,\n",
        "                            input_length=maxlen,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=False)(input1_post)\n",
        "    embed_layer1_post = SpatialDropout1D(0.4)(embed_layer1_post)\n",
        "\n",
        "    x_post = Bidirectional(CuDNNGRU(128, return_sequences=True))(embed_layer1_post)\n",
        "    capsule_post1 = PrimaryCap(x_post,16,1,1)\n",
        "    capsule_post2 = PrimaryCap(x_post,16,1,3)\n",
        "    capsule_post3 = PrimaryCap(x_post,16,1,5)\n",
        "    toxiccaps_post1 = CapsuleLayer(num_capsule=6, dim_capsule=16, routings=7, name='toxiccapspost1')(capsule_post1)\n",
        "    toxiccaps_post2 = CapsuleLayer(num_capsule=6, dim_capsule=16, routings=7, name='toxiccapspost2')(capsule_post2)\n",
        "    toxiccaps_post3 = CapsuleLayer(num_capsule=6, dim_capsule=16, routings=7, name='toxiccapspost3')(capsule_post3)\n",
        "#     capsule_post = Capsule(num_capsule=10, dim_capsule=16, routings=5,share_weights=True)(x_post)\n",
        "    #  output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
        "    toxiccaps_post = concatenate([toxiccaps_post1,toxiccaps_post2,toxiccaps_post3])\n",
        "    capsule_post = GlobalMaxPooling1D()(toxiccaps_post)\n",
        "    capsule_post = Dropout(0.25)(capsule_post)\n",
        "\n",
        "    concat = concatenate([capsule_pre,capsule_post])\n",
        "    output = Dense(6, activation='sigmoid')(concat)\n",
        "\n",
        "    model = Model(inputs=[input1_pre,input1_post], outputs=output)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(lr=1e-3,decay=0),\n",
        "        metrics=['accuracy'])\n",
        "#     model.summary()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_proba(arr,TC=TC_W):\n",
        "    import numpy as np\n",
        "    from tensorflow.python.keras.preprocessing import sequence\n",
        "    pred=TC.model.predict(sequence.pad_sequences(TC.token.texts_to_sequences(arr),maxlen=TC.MAX_SEQ_LEN))\n",
        "    returnable=[]\n",
        "    for i in pred:\n",
        "        temp=i[0]\n",
        "        returnable.append(np.array([1-temp,temp]))\n",
        "    return np.array(returnable)\n",
        "\n",
        "\n",
        "def error_analysis(TC,model,Y_pred):\n",
        "    import numpy as np\n",
        "    from IPython.display import display\n",
        "    import matplotlib.pyplot as plt\n",
        "    import eli5\n",
        "    from eli5.lime import TextExplainer\n",
        "    import shap\n",
        "    from lime.lime_text import LimeTextExplainer\n",
        "    import tensorflow.compat.v1 as tf\n",
        "    tf.disable_v2_behavior()\n",
        "    from tensorflow.python.keras.preprocessing import sequence\n",
        "\n",
        "    import tensorflow.keras.backend as K\n",
        "    #token = Tokenizer(num_words=self.LEN_VOCAB)\n",
        "    #token.fit_on_texts(self.X_tr)\n",
        "    lime_explainer= LimeTextExplainer(class_names=[0,1])\n",
        "\n",
        "    te = TextExplainer(random_state=0)\n",
        "    distrib_samples = TC.X_train[:100]\n",
        "    #explainer = shap.DeepExplainer(model, distrib_samples)\n",
        "    # explain the first 25 predictions\n",
        "    # explaining each prediction requires 2 * background dataset size runs\n",
        "    # num_explanations = 25 #len(TC_F.X_test)\n",
        "    # shap_values = explainer.shap_values(TC.X_test[:num_explanations])\n",
        "    # shap.initjs()\n",
        "    # num2word = {}\n",
        "    arr_index=TC.X_te.index\n",
        "    # for w in TC.word_index.keys():\n",
        "    #     num2word[TC.word_index[w]] = w\n",
        "    # x_test_words = np.stack([np.array(list(map(lambda x: num2word.get(x, \"NONE\"), TC.X_test[i]))) for i in range(num_explanations)])\n",
        "    i=0\n",
        "    \n",
        "    for s in TC.X_te:\n",
        "        if(i==21):\n",
        "            break\n",
        "        if(TC.Y_test[arr_index[i]] != Y_pred[i]):\n",
        "            print(s)\n",
        "            print(\"Predicted Label : \",TC.result_map(Y_pred[i]),\" | Turth Label : \",TC.result_map(TC.Y_test[arr_index[i]]))\n",
        "            te.fit(TC.X_te[arr_index[i]],predict_proba)\n",
        "            #shap.force_plot(explainer.expected_value[0], shap_values[0][i], x_test_words[i],matplotlib=True)\n",
        "            display(te.show_prediction(target_names=[0,1]))\n",
        "            display(lime_explainer.explain_instance(TC.X_te[arr_index[i]],predict_proba).show_in_notebook(text=True))\n",
        "        i+=1\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def caps_model1(TC):\n",
        "    # from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
        "    inp = Input( shape=(TC.MAX_SEQ_LEN,))\n",
        "    if(TC.EMBEDDING == None):\n",
        "        embedding_layer = Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
        "                        input_dim=TC.LEN_VOCAB, \n",
        "                        input_length=TC.MAX_SEQ_LEN,\n",
        "                        #weights=[TC.emb_matrix], # Additionally we give the Wi\n",
        "                        trainable=TC.trainable)(inp)\n",
        "    else:\n",
        "        embedding_layer = Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
        "                            input_dim=TC.LEN_VOCAB, \n",
        "                            input_length=TC.MAX_SEQ_LEN,\n",
        "                            weights=[TC.emb_matrix], # Additionally we give the Wi\n",
        "                            trainable=TC.trainable)(inp)\n",
        "    #embedding_layer = SpatialDropout1D(rate_drop_dense)(embedding_layer)\n",
        "    \n",
        "    #capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True)(x)\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to \n",
        "    # kernel_size should be smaller than window size... maybe half of window size?\n",
        "    # [None, num_capsule, dim_vector]\n",
        "    conv1 = layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(embedding_layer)\n",
        "    primarycaps = PrimaryCap( conv1, dim_capsule=dim_capsule, \n",
        "                            n_channels=1, kernel_size=1,strides=1, padding='valid')\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    caps = CapsuleLayer( num_capsule=num_capsule, dim_capsule=dim_capsule, routings=num_routing, name='caps')(primarycaps)\n",
        "\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. \n",
        "    # Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    print( \"ner_caps\", caps.get_shape())\n",
        "    #out_caps = Length(name='out_caps')(caps)\n",
        "    #print( \"out_caps\", out_caps.get_shape())\n",
        "\n",
        "    capsule = Flatten()(capsule)\n",
        "    caps = Dropout(dropout_p)(caps)\n",
        "    output = Dense(1, activation=\"sigmoid\")(caps)\n",
        "    model = Model(inputs=inp, outputs=output)\n",
        "    optimizer_adam = Adam(learning_rate=TC.lr)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optimizer_adam,\n",
        "                    metrics=['acc'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Fasttext trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load data\n",
            "\n",
            "X train (4526,) Y train (4526,) X test (1940,) Y test (1940,)\n",
            "Build vocab\n",
            "\n",
            "Load Embedding model\n",
            "\n",
            "Create embedding index\n",
            "\n",
            "found 818830 word vectors\n",
            "Check coverage\n",
            "Found embeddings for 87.50% of vocab\n",
            "Found embeddings for  96.39% of all text\n",
            "Added 2197 words to embedding with rare words handling of fasttext\n",
            "Check coverage\n",
            "Found embeddings for 100.00% of vocab\n",
            "Found embeddings for  100.00% of all text\n",
            "dictionary size:  17577\n",
            "Embedding matrix \n",
            "\n"
          ]
        }
      ],
      "source": [
        "TC_F = TextClassifier.TextClassifier(EMBEDDING='fasttext')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tcn_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-22-2434c3edd511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mTC_F\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" CapsNet trainable fasttext\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mTC_F\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtcn_tf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtcn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTC_F\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtcn_tf_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTC_F\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtcn_tf_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTC_F\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtcn_tf_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tcn_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Fasttext dynamic embedding - tranable = True \n",
        "TC_F.EPOCHS =50\n",
        "TC_F.BATCH_SIZE =16\n",
        "TC_F.lr = 0.0001\n",
        "TC_F.tag = \" CapsNet trainable fasttext\"\n",
        "TC_F.trainable = True\n",
        "tcn_tf_model = tcn_model(TC_F)\n",
        "tcn_tf_model,hist = TC_F.train_model(tcn_tf_model)\n",
        "Y_pred = TC_F.model_evaluate(tcn_tf_model,hist)\n",
        "TC_F.save_predictions(Y_pred,TC_F.tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(?, 92, 6)\n",
            "squash\n",
            "92\n",
            "Tensor(\"caps_25/zeros:0\", shape=(?, 4, 1, 92), dtype=float32)\n",
            "Tensor(\"caps_25/zeros_1:0\", shape=(?, 4, 1, 92), dtype=float32)\n",
            "i = 0\n",
            "b loop= (?, 4, 1, 92)\n",
            "c shape (?, 4, 1, 92)\n",
            "input hat -  <unknown>\n",
            "before squash s <unknown>\n",
            "before squash <unknown>\n",
            "squash\n",
            " output =  <unknown>\n",
            "inside if\n",
            "b = Tensor(\"caps_25/add_2:0\", dtype=float32)\n",
            "b shape  <unknown>\n",
            "i = 1\n",
            "b loop= <unknown>\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    <ipython-input-87-fcdb3617ab99>:149 call  *\n        c = tf.nn.softmax(b, axis=1)\n    C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3734 softmax_v2\n        return _wrap_2d_function(logits, gen_nn_ops.softmax, axis, name)\n    C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3617 _wrap_2d_function\n        is_last_dim = (dim == -1) or (dim == shape.ndims - 1)\n\n    TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-88-7ac78fbe8fbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#cps_f_model = build_model(TC_F)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcps_f_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaps_model1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTC_F\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mcps_f_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTC_F\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcps_f_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTC_F\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcps_f_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-77-e0c168adf2e1>\u001b[0m in \u001b[0;36mcaps_model1\u001b[1;34m(TC)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Layer 3: Capsule layer. Routing algorithm works here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mcaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCapsuleLayer\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnum_capsule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_capsule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_capsule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdim_capsule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroutings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_routing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'caps'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimarycaps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m               with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m    785\u001b[0m                   self._compute_dtype_object):\n\u001b[1;32m--> 786\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-87-fcdb3617ab99>:149 call  *\n        c = tf.nn.softmax(b, axis=1)\n    C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3734 softmax_v2\n        return _wrap_2d_function(logits, gen_nn_ops.softmax, axis, name)\n    C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3617 _wrap_2d_function\n        is_last_dim = (dim == -1) or (dim == shape.ndims - 1)\n\n    TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n"
          ]
        }
      ],
      "source": [
        "# increased dropout to 0.2\n",
        "TC_F.EPOCHS =2\n",
        "TC_F.BATCH_SIZE =16\n",
        "TC_F.lr = 0.0001\n",
        "TC_F.tag = \" CapsNet fasttext\"\n",
        "TC_F.trainable = False\n",
        "\n",
        "#cps_f_model = build_model(TC_F)\n",
        "cps_f_model = caps_model1(TC_F)\n",
        "cps_f_model,hist = TC_F.train_model(cps_f_model)\n",
        "Y_pred = TC_F.model_evaluate(cps_f_model,hist)\n",
        "TC_F.save_predictions(Y_pred,TC_F.tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class TextCapsule(object):\n",
        "#     def __init__(self, training,params):\n",
        "#         self.training = training\n",
        "#         self.params = params\n",
        "#         self.embedding_layer = EmbeddingLayer(vocab_size=params['vocab_size'],\n",
        "#                                                embed_size=params['embedding_size'],\n",
        "#                                                embedding_type=params['embedding_type'],\n",
        "#                                                params=params)\n",
        "#         self.capsule_layer_conv=Capsule_layer_conv(shape=[3,1,64,64],vec_length=self.params['capsule_vec_length'])\n",
        "#         self.capsule_layer_dense=Capsule_layer_dense(hidden_size=params['n_class'])\n",
        "\n",
        "#     def build(self, inputs):\n",
        "#         with tf.name_scope('embed'):\n",
        "#             embedding_outputs = self.embedding_layer(inputs)\n",
        "#         if self.training:\n",
        "#             embedding_outputs = tf.nn.dropout(embedding_outputs, self.params['embedding_dropout_keep'])\n",
        "\n",
        "#         embedding_outputs=tf.expand_dims(embedding_outputs,-1)\n",
        "#         with tf.name_scope('conv'):\n",
        "#             conv1 = tf.layers.conv2d(inputs=embedding_outputs,\n",
        "#                                      filters=self.params['filters'],\n",
        "#                                      kernel_size=[3,300],\n",
        "#                                      strides=1,\n",
        "#                                      padding='valid',\n",
        "#                                      activation=tf.nn.relu) #[batch_size,seq_length-2,1,128]\n",
        "\n",
        "#         cap_conv=self.capsule_layer_conv(conv1)\n",
        "#         cap_conv_shape=cap_conv.get_shape().as_list()\n",
        "#         #cap_flat shape [batch_size, (seq_length-4)* cap_kernel_size[-1]), vec_length]\n",
        "#         cap_flat=tf.reshape(cap_conv,\n",
        "#                             [-1,cap_conv_shape[1]*cap_conv_shape[2]*cap_conv_shape[3],self.capsule_layer_conv.vec_length])\n",
        "#         logits=self.capsule_layer_dense(cap_flat)\n",
        "#         return logits\n",
        "\n",
        "#     def __call__(self, inputs, targets=None):\n",
        "#         logits=self.build(inputs)\n",
        "#         return logits\n",
        "def caps_model2(TC):\n",
        "    # from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
        "    inp = Input( shape=(TC.MAX_SEQ_LEN,))\n",
        "    if(TC.EMBEDDING == None):\n",
        "        embedding_layer = Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
        "                        input_dim=TC.LEN_VOCAB, \n",
        "                        input_length=TC.MAX_SEQ_LEN,\n",
        "                        #weights=[TC.emb_matrix], # Additionally we give the Wi\n",
        "                        trainable=TC.trainable)(inp)\n",
        "    else:\n",
        "        embedding_layer = Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
        "                            input_dim=TC.LEN_VOCAB, \n",
        "                            input_length=TC.MAX_SEQ_LEN,\n",
        "                            weights=[TC.emb_matrix], # Additionally we give the Wi\n",
        "                            trainable=TC.trainable)(inp)\n",
        "    #embedding_layer = SpatialDropout1D(rate_drop_dense)(embedding_layer)\n",
        "    \n",
        "    #capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,share_weights=True)(x)\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to \n",
        "    # kernel_size should be smaller than window size... maybe half of window size?\n",
        "    # [None, num_capsule, dim_vector]\n",
        "    conv1 = layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(embedding_layer)\n",
        "    primarycaps = PrimaryCap( conv1, dim_capsule=dim_capsule, \n",
        "                            n_channels=1, kernel_size=1,strides=1, padding='valid')\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    caps = CapsuleLayer( num_capsule=num_capsule, dim_capsule=dim_capsule, routings=num_routing, name='caps')(primarycaps)\n",
        "\n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. \n",
        "    # Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    print( \"ner_caps\", caps.get_shape())\n",
        "    #out_caps = Length(name='out_caps')(caps)\n",
        "    #print( \"out_caps\", out_caps.get_shape())\n",
        "\n",
        "    capsule = Flatten()(capsule)\n",
        "    caps = Dropout(dropout_p)(caps)\n",
        "    output = Dense(1, activation=\"sigmoid\")(caps)\n",
        "    model = Model(inputs=inp, outputs=output)\n",
        "    optimizer_adam = Adam(learning_rate=TC.lr)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                    optimizer=optimizer_adam,\n",
        "                    metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "class Capsule_layer(tf.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Capsule_layer,self).__init__()\n",
        "\n",
        "    def squash(self,vector,axis=-1,epsilon=1e-7):\n",
        "        vec_squared_norm=tf.reduce_sum(tf.square(vector),axis=axis,keepdims=True)\n",
        "        scale=vec_squared_norm/(1+vec_squared_norm)/tf.sqrt(vec_squared_norm+epsilon)\n",
        "        return scale*vector\n",
        "\n",
        "    def dynamic_routing(self,input,iters):\n",
        "        # input shape [batch_size,n_class, (seq_length-4)* cap_kernel_size[-1]), vec_length]\n",
        "        # output shape [batch_size, n_class, vec_length]\n",
        "        b=tf.zeros_like(input[:,:,:,-1], dtype=tf.float32, name='b')\n",
        "        for i in range(iters):\n",
        "            c=tf.nn.softmax(b,1)\n",
        "            output=self.squash(tf.einsum('bij,bijk->bik',c,input))\n",
        "            if i<iters-1:\n",
        "                b=b+tf.einsum('bik,bijk->bij',output,input)\n",
        "        return output\n",
        "\n",
        "    def vec_transform_conv(self,inputs,input_cap_dim,input_cap_num,output_cap_dim,output_cap_num):\n",
        "        #output u_hat_vecs shape:[batch_size, hidden_size,(seq_length-4)* cap_kernel_size[-1]),vec_length]\n",
        "        kernel_size=[1]\n",
        "        u_hat_vecs=tf.layers.conv1d(inputs=inputs,kernel_size=kernel_size,filters=output_cap_dim*output_cap_num)\n",
        "        u_hat_vecs=tf.reshape(u_hat_vecs,[-1,input_cap_num,output_cap_num,output_cap_dim])\n",
        "        u_hat_vecs=tf.transpose(u_hat_vecs,(0,2,1,3))\n",
        "        return u_hat_vecs\n",
        "\n",
        "\n",
        "class Capsule_layer_conv(Capsule_layer):\n",
        "    def __init__(self,shape,vec_length):\n",
        "        super(Capsule_layer_conv,self).__init__()\n",
        "        self.shape=shape\n",
        "        self.vec_length=vec_length\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        #output cap shape #[batch_size,seq_length-4,1,shape[-1], vec_length]\n",
        "        cap=tf.layers.conv2d(inputs,filters=self.shape[-1]*self.vec_length,kernel_size=self.shape[:2],strides=1)\n",
        "        cap_shape=cap.get_shape().as_list()\n",
        "        cap=tf.reshape(cap,[-1,cap_shape[1],cap_shape[2],self.shape[-1],self.vec_length])\n",
        "        cap=self.squash(cap)\n",
        "        return cap\n",
        "\n",
        "\n",
        "class Capsule_layer_dense(Capsule_layer):\n",
        "    def __init__(self,hidden_size):\n",
        "        super(Capsule_layer_dense,self).__init__()\n",
        "        self.hidden_size=hidden_size\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        inputs_shape=inputs.get_shape().as_list()\n",
        "        cap=self.vec_transform_conv(inputs,input_cap_dim=inputs_shape[-1],input_cap_num=inputs_shape[1],\n",
        "                                    output_cap_dim=inputs_shape[-1],output_cap_num=self.hidden_size)\n",
        "        outputs=self.dynamic_routing(cap,iters=3)\n",
        "        #outputs shape [batch_size, n_class]\n",
        "        outputs=tf.sqrt(tf.reduce_sum(tf.square(outputs),axis=2))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# The following is another way to implement primary capsule layer. This is much slower.\\n# Apply Conv2D `n_channels` times and concatenate all capsules\\ndef PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\\n    outputs = []\\n    for _ in range(n_channels):\\n        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\\n        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\\n    outputs = layers.Concatenate(axis=1)(outputs)\\n    return layers.Lambda(squash)(outputs)\\n'"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, \n",
        "not just on MNIST.\n",
        "*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\n",
        "uncommenting them and commenting their counterparts.\n",
        "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, layers\n",
        "\n",
        "\n",
        "class Length(layers.Layer):  # CapsuleNorm\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=x.shape[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    print(\"squash\")\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "        print(self.input_num_capsule)\n",
        "        # Transform matrix, from each input capsule to each output capsule, there's a unique weight as in Dense layer.\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_expand = tf.expand_dims(tf.expand_dims(inputs, 1), -1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_tiled = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
        "        # Regard the first two dimensions as `batch` dimension, then\n",
        "        # matmul(W, x): [..., dim_capsule, input_dim_capsule] x [..., input_dim_capsule, 1] -> [..., dim_capsule, 1].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        inputs_hat = tf.squeeze(tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled))\n",
        "\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, 1, self.input_num_capsule].\n",
        "        \n",
        "        print(tf.zeros(shape=[K.shape(inputs)[0], self.num_capsule, 1, self.input_num_capsule]))\n",
        "        b = tf.zeros(shape=[K.shape(inputs)[0], self.num_capsule, 1, self.input_num_capsule])\n",
        "    \n",
        "        print(b)\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            print(\"i =\",i)\n",
        "            # c.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "            print('b loop=',b.get_shape())\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "            print(\"c shape\",c.shape)\n",
        "            print('input hat - ',inputs_hat.shape)\n",
        "            # c.shape = [batch_size, num_capsule, 1, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [..., 1, input_num_capsule] x [..., input_num_capsule, dim_capsule] -> [..., 1, dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, 1, dim_capsule]\n",
        "            #s = tf.einsum('bij,bijk->bik',c,inputs_hat)\n",
        "            s = tf.matmul(c, inputs_hat)\n",
        "            print(\"before squash s\",s.shape)\n",
        "            print(\"before squash\",s.shape)\n",
        "            outputs = squash(s)  # [None, 10, 1, 16]\n",
        "            print(\" output = \",outputs.shape)\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, 1, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension, then\n",
        "                # matmal:[..., 1, dim_capsule] x [..., input_num_capsule, dim_capsule]^T -> [..., 1, input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, 1, input_num_capsule]\n",
        "                print(\"inside if\")\n",
        "                b += tf.matmul(outputs, inputs_hat, transpose_b=True)\n",
        "                print(\"b =\",b)\n",
        "                print(\"b shape \",b.shape)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return tf.squeeze(outputs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    output = layers.Conv1D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d')(inputs)\n",
        "    #output = layers.Conv1D(filters=dim_capsule*n_channels, kernel_size=kernel_size,name='primarycap_conv1d')(inputs)    \n",
        "    print(output.shape)                  \n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
        "   # return layers.Lambda(squash)(output)\n",
        "\n",
        "\"\"\"\n",
        "# The following is another way to implement primary capsule layer. This is much slower.\n",
        "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    outputs = []\n",
        "    for _ in range(n_channels):\n",
        "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
        "    outputs = layers.Concatenate(axis=1)(outputs)\n",
        "    return layers.Lambda(squash)(outputs)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_analysis(TC_F,tcn_f_model,Y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### word2vec skipgram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TC_W = TextClassifier.TextClassifier(EMBEDDING='w2v_skipgram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# increase dropout to 0.2\n",
        "TC_W.EPOCHS =50\n",
        "TC_W.BATCH_SIZE =16\n",
        "TC_W.lr = 0.0001\n",
        "TC_W.tag = \"CapsNet w2v skipgram \"\n",
        "tcn_w_model = build_model(TC_W)\n",
        "tcn_w_model, hist = TC_W.train_model(tcn_w_model)\n",
        "Y_pred = TC_W.model_evaluate(tcn_w_model,hist)\n",
        "TC_W.save_predictions(Y_pred,TC_W.tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_analysis(TC_W,tcn_w_model,Y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### word2vec cbow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TC_W_2 = TextClassifier.TextClassifier(EMBEDDING='w2v_cbow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dropout 0.2\n",
        "TC_W_2.EPOCHS =50\n",
        "TC_W_2.BATCH_SIZE =16\n",
        "TC_W_2.lr = 0.0001\n",
        "TC_W_2.tag = \"CapsNet w2v cbow\"\n",
        "tcn_w2_model = build_model(TC_W_2)\n",
        "\n",
        "tcn_w2_model, hist = TC_W_2.train_model(tcn_w2_model)\n",
        "Y_pred = TC_W_2.model_evaluate(tcn_w2_model,hist)\n",
        "TC_W_2.save_predictions(Y_pred,TC_W_2.tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_analysis(TC_W_2,tcn_w2_model,Y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Random embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load data\n",
            "\n",
            "X train (4526,) Y train (4526,) X test (1940,) Y test (1940,)\n",
            "dictionary size:  17577\n"
          ]
        }
      ],
      "source": [
        "TC = TextClassifier.TextClassifier(EMBEDDING=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fasttext dynamic embedding - tranable = True - changesd architecture\n",
        "TC.EPOCHS =50\n",
        "TC.BATCH_SIZE =16\n",
        "TC.lr = 0.0001\n",
        "TC.tag = \"CapsNet random emb\"\n",
        "TC.trainable = True\n",
        "tcn_tf_model = build_model(TC)\n",
        "tcn_tf_model,hist = TC.train_model(tcn_tf_model)\n",
        "Y_pred = TC.model_evaluate(tcn_tf_model,hist)\n",
        "TC.save_predictions(Y_pred,TC.tag)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "RNN variant Fasttext.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
