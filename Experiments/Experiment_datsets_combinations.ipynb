{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Planned Experiments\n",
    "\n",
    "1. Model LR with n-gram (bow) weigted by TF-IDF and apply individual dataset seperatly\n",
    "2. Apply LR with n-gram (bow) weigted by TF-IDF and apply to gossip + kaggle datasets\n",
    "3. Apply LR with n-gram (bow) weigted by TF-IDF and apply to gossip + Sinhala_singlish datasets\n",
    "4. Apply LR with n-gram (bow) weigted by TF-IDF and apply to gossip + Twiiter datasets\n",
    "\n",
    "Next step -> check same LR model with word2vec or any embedding techniques\n",
    "\n",
    "Next step -> check datasets with Deep learning model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Plan Experimnet settings with Sacred and Neptune"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(project=NEPTUNE_PROJECT)\n",
    "# Create run in project\n",
    "run = neptune.init(api_token=NEPTUNE_API_TOKEN)              \n",
    "run[\"JIRA\"] = \"NPT-952\"\n",
    "run[\"parameters\"] = {\"learning_rate\": 0.001,\n",
    "                     \"optimizer\": \"Adam\"}\n",
    "\n",
    "for epoch in range(100):\n",
    "   run[\"train/loss\"].log(epoch * 0.4)\n",
    "run[\"eval/f1_score\"] = 0.66"
   ]
  },
  {
   "source": [
    "## Import modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score,roc_curve,roc_auc_score,confusion_matrix,recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import helper function script\n",
    "import sys\n",
    "sys.path.insert(1,'G:\\\\Github\\\\Sinhala-Hate-Speech-Detection')\n",
    "import utills"
   ]
  },
  {
   "source": [
    "## Read datasets to dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = d.read_csv ('../Datasets/raw/gossip_dataset_complete_v0.csv',header =None)\n",
    "df.columns = ['comment','label']  # gossip dataset\n",
    "df_B = pd.read_csv('../Datasets/raw/kaggle_dataset.csv',header =None)    # fb dataset -kaggle\n",
    "print(df_B.head())\n",
    "df_c = pd.read_csv('../Datasets/raw/twitter_dataset.csv',header =None)    # multiple_domain dataset\n",
    "print(df_C.head())\n",
    "df_D =  pd.read_csv('../Datasets/raw/Sinhala_Singlish_Hate_Speech.csv')   # Twitter dataset -github\n",
    "print(df_D.head())"
   ]
  },
  {
   "source": [
    "## Prepare datasets for experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make same columns and labels give new col representing dataset"
   ]
  },
  {
   "source": [
    "## Make pair of datsets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    "
   ]
  },
  {
   "source": [
    "## Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram bag of words\n",
    "\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "\n",
    "#  word2vec"
   ]
  },
  {
   "source": [
    "## Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(df,feature):\n",
    "    #print classification report \n",
    "    #ROC cureve + AUC score\n",
    "def LSTM(df,feature):\n",
    "    #print classificaation report\n",
    "    #ROC cureve + AUC score\n",
    "\n",
    "# def svm()\n",
    "\n",
    "# def CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def evaluation():"
   ]
  },
  {
   "source": [
    "## Dataset Check"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset name list\n",
    "df_list = []\n",
    "\n",
    "def dataset_check(df_list):\n",
    "    #results = {}\n",
    "    for df in df_list:\n",
    "        preprocess()\n",
    "        feature()\n",
    "        LR()\n",
    "        LSTM()\n",
    "        #evaluation()\n",
    "        # result {[df_name :{f1score:,accuracy,recall:,precision:, AUC socre}]}\n",
    "        #results[df].append(result)\n"
   ]
  },
  {
   "source": [
    "## Compare Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}