{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "embeddings_index = {}\r\n",
    "f = open('../input/glove840b300dtxt/glove.840B.300d.txt')\r\n",
    "for line in tqdm(f):\r\n",
    "    values = line.split()\r\n",
    "    word = values[0]\r\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
    "    embeddings_index[word] = coefs\r\n",
    "f.close()\r\n",
    "\r\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this function creates a normalized vector for the whole sentence\r\n",
    "def sent2vec(s):\r\n",
    "    words = str(s).lower().decode('utf-8')\r\n",
    "    words = word_tokenize(words)\r\n",
    "    words = [w for w in words if not w in stop_words]\r\n",
    "    words = [w for w in words if w.isalpha()]\r\n",
    "    M = []\r\n",
    "    for w in words:\r\n",
    "        try:\r\n",
    "            M.append(embeddings_index[w])\r\n",
    "        except:\r\n",
    "            continue\r\n",
    "    M = np.array(M)\r\n",
    "    v = M.sum(axis=0)\r\n",
    "    if type(v) != np.ndarray:\r\n",
    "        return np.zeros(300)\r\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create sentence vectors using the above function for training and validation set\r\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\r\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\r\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# scale the data before any neural net:\r\n",
    "scl = preprocessing.StandardScaler()\r\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\r\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "# using keras tokenizer here\r\n",
    "token = text.Tokenizer(num_words=None)\r\n",
    "max_len = 70\r\n",
    "\r\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\r\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\r\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\r\n",
    "\r\n",
    "# zero pad the sequences\r\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\r\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\r\n",
    "\r\n",
    "word_index = token.word_index\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create an embedding matrix for the words we have in the dataset\r\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\r\n",
    "for word, i in tqdm(word_index.items()):\r\n",
    "    embedding_vector = embeddings_index.get(word)\r\n",
    "    if embedding_vector is not None:\r\n",
    "        embedding_matrix[i] = embedding_vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(len(word_index) + 1,\r\n",
    "                     300,\r\n",
    "                     weights=[embedding_matrix],\r\n",
    "                     input_length=max_len,\r\n",
    "                     trainable=False))\r\n",
    "model.add(SpatialDropout1D(0.3))\r\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\r\n",
    "\r\n",
    "model.add(Dense(1024, activation='relu'))\r\n",
    "model.add(Dropout(0.8))\r\n",
    "\r\n",
    "model.add(Dense(1024, activation='relu'))\r\n",
    "model.add(Dropout(0.8))\r\n",
    "\r\n",
    "model.add(Dense(3))\r\n",
    "model.add(Activation('softmax'))\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(len(word_index) + 1,\r\n",
    "                     300,\r\n",
    "                     weights=[embedding_matrix],\r\n",
    "                     input_length=max_len,\r\n",
    "                     trainable=False))\r\n",
    "model.add(SpatialDropout1D(0.3))\r\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\r\n",
    "\r\n",
    "model.add(Dense(1024, activation='relu'))\r\n",
    "model.add(Dropout(0.8))\r\n",
    "\r\n",
    "model.add(Dense(1024, activation='relu'))\r\n",
    "model.add(Dropout(0.8))\r\n",
    "\r\n",
    "model.add(Dense(3))\r\n",
    "model.add(Activation('softmax'))\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
    "\r\n",
    "# Fit the model with early stopping callback\r\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\r\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \r\n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "# GRU with glove embeddings and two dense layers\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(len(word_index) + 1,\r\n",
    "                     300,\r\n",
    "                     weights=[embedding_matrix],\r\n",
    "                     input_length=max_len,\r\n",
    "                     trainable=False))\r\n",
    "model.add(SpatialDropout1D(0.3))\r\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\r\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\r\n",
    "\r\n",
    "model.add(Dense(1024, activation='relu'))\r\n",
    "model.add(Dropout(0.8))\r\n",
    "\r\n",
    "model.add(Dense(1024, activation='relu'))\r\n",
    "model.add(Dropout(0.8))\r\n",
    "\r\n",
    "model.add(Dense(3))\r\n",
    "model.add(Activation('softmax'))\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
    "\r\n",
    "# Fit the model with early stopping callback\r\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\r\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \r\n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}