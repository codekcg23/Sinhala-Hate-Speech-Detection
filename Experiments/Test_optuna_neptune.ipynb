{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogQ-wIb5KhYH"
   },
   "source": [
    "### Experiment : Set up optuna with neptune for Hyper parameter and architecture buildinng\n",
    "### Objectives\n",
    "\n",
    "1. batch size =[8,16,32]\n",
    "2. learining rate = [0.1,0.01,0.0001,0.00001]\n",
    "3. units = [8,16,32,64,128]\n",
    "4. number of layers in architecture [4,5,6]\n",
    "5. optimization = [adam,]\n",
    "6. epochs = [10,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ikmcc1WMKgNK"
   },
   "source": [
    "### Observations\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF8BkDzFKg9f"
   },
   "source": [
    "### Solutions and further investigation\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2xtbDAWLGVd"
   },
   "source": [
    "### Load Modeules and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from keras import layers\n",
    "from sklearn import metrics\n",
    "from keras import regularizers\n",
    "from keras import models,optimizers\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from keras.layers import Dense,Input,SpatialDropout1D,Dropout,Flatten, SimpleRNN,LSTM,RNN,GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import re\n",
    "import gensim\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "# import helper function script\n",
    "import sys\n",
    "sys.path.insert(1,'G:\\\\Github\\\\Sinhala-Hate-Speech-Detection')\n",
    "import utills\n",
    "import sinhala_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "#neptune_cbk = NeptuneCallback(run=run, base_namespace='metrics')\n",
    "#run.stop()\n",
    "#run = neptune.init(project_qualified_name= NEPTUNE_PROJECT,api_token=NEPTUNE_API_TOKEN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "#import neptune.new as neptune\n",
    "from neptunecontrib.monitoring.metrics import expand_prediction, log_class_metrics, log_binary_classification_metrics, log_classification_report,log_confusion_matrix,log_prediction_distribution\n",
    "from neptunecontrib.api import log_table\n",
    "import neptunecontrib.monitoring.optuna as opt_utils \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "NEPTUNE_PROJECT= os.getenv('NEPTUNE_PROJECT')\n",
    "NEPTUNE_API_TOKEN = os.getenv(('NEPTUNE_API_TOKEN'))\n",
    "neptune.init(project_qualified_name= NEPTUNE_PROJECT,api_token=NEPTUNE_API_TOKEN) \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets - no preprocessing\n",
    "path = '../Datasets/processed/no_stemming/'\n",
    "df_A = pd.read_csv(path+'df_A.csv')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN entries in dataset\n",
    "df_A.drop(index =[610,3070],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_model(emb_model):\n",
    "    from gensim.models import word2vec\n",
    "    import gensim.models as FastText\n",
    "    from gensim.test.utils import datapath\n",
    "    \n",
    "    if emb_model == \"w2v_skipgram\":\n",
    "        model = word2vec.Word2Vec.load(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/word2vec/word2vec_300.w2v\")\n",
    "    elif emb_model == \"w2v_cbow\":\n",
    "        model = word2vec.Word2Vec.load(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/CBOW-word2vec/cbow_300.w2v\")\n",
    "    elif emb_model == \"fasttext\":\n",
    "        #FastText.load_fasttext_format(\"../../../corpus/analyzed/saved_models/wiki.si.bin\")\n",
    "        model = FastText.fasttext.load_facebook_model(datapath(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/cc.si.300.bin\"))\n",
    "        #model = word2vec.Word2Vec.load(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/fasttext_300.w2v\")\n",
    "    else:\n",
    "        print(\"Invalid argument. Need w2v_skipgram or w2v_cbow or fasttext as argument\")\n",
    "        model = None\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_index(model):\n",
    "    from gensim.models import word2vec\n",
    "    embeddings_index ={}\n",
    "    for index, word in enumerate(model.wv.index_to_key):\n",
    "        embeddings_index[word] = model.wv.get_vector(word)\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_OOV(model,oov_vocab,embed_index):\n",
    "    count = 0\n",
    "    for word, freq in oov_vocab:\n",
    "        if word in model.wv:\n",
    "            embed_index[word] = model.wv.get_vector(word)\n",
    "            count+=1\n",
    "    print(f\"Added {count} words to embedding with rare words handling of fasttext\")\n",
    "    return embed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext = load_emb_model(\"fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df_A['cleaned'])\n",
    "embed_fasttext = get_emb_index(model_fasttext)\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "emb_index_fasttext =fasttext_OOV(model_fasttext,oov_fasttext,embed_fasttext)\n",
    "check_coverage(vocab, emb_index_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_VOCAB = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 100\n",
    "def max_len(df):\n",
    "  train_df = pd.DataFrame()\n",
    "  train_df['doc_len'] = df.apply(lambda words: len(words.split()))\n",
    "  MAX_SEQ_LEN = max(train_df['doc_len'])\n",
    "  #MAX_SEQ_LEN = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "  print(\"max seq len\",MAX_SEQ_LEN)\n",
    "  return MAX_SEQ_LEN\n",
    "\n",
    "# create a tokenizer \n",
    "def create_sequence(X_train,X_test,LEN_VOCAB):\n",
    "\n",
    "  token = Tokenizer(num_words=LEN_VOCAB)\n",
    "  token.fit_on_texts(X_train)\n",
    "  word_index = token.word_index\n",
    "  print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "  # ensure equal length vectors \n",
    "  train_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=MAX_SEQ_LEN)\n",
    "  test_seq_x = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=MAX_SEQ_LEN)\n",
    "  return (train_seq_x,test_seq_x,word_index)\n",
    "\n",
    "# create token-embedding mapping for \n",
    "def word_mapping(word_index):\n",
    "    words_not_found=[]\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    # embedding_matrix = np.random.uniform(-0.25, 0.25, size=(vocab, self.EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: #and len(embedding_vector) > 0\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.randn(300)\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    #print(\"sample words not found: \", np.random.choice(words_not_found, 200))\n",
    "    print(\"Number of words not found\",len(words_not_found))\n",
    "    print(\"Propotion of oov \",(len(words_not_found)/len(word_index))*100)\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix(embeddings_index, word_index, len_voc):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_seq,test_seq,Y_train,Y_test,tag):\n",
    "  \n",
    "  #define callbacks\n",
    "  early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1,mode='min',restore_best_weights=True)\n",
    "  #callbacks_list = [early_stopping]\n",
    "\n",
    "  #model training\n",
    "  print(\"started training\")\n",
    "  hist = model.fit(train_seq, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[early_stopping],validation_split=0.1, shuffle=False, verbose=2)\n",
    "  train_loss, train_acc = model.evaluate(train_seq, Y_train,batch_size=BATCH_SIZE, verbose=1)\n",
    "  print(\"train loss - \",train_loss,\" train acc- \",train_acc)\n",
    "  test_loss, test_acc = model.evaluate(test_seq,Y_test,batch_size=BATCH_SIZE, verbose=1)\n",
    "  print(\"test loss - \",test_loss,\" test acc- \",test_acc)\n",
    "\n",
    "  # plot loss during training\n",
    "  #from matplotlib import pyplot\n",
    "  print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "  plt.subplot(211)\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel(\"Cross Entropy Loss\")\n",
    "  plt.plot(hist.history['loss'],lw=2.0, color='b', label='train')\n",
    "  plt.plot(hist.history['val_loss'],lw=2.0, color='r', label='validation')\n",
    "  plt.legend(loc='upper right')\n",
    "  # plot accuracy during training\n",
    "  plt.subplot(212)\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.plot(hist.history['accuracy'],lw=2.0, color='b', label='train')\n",
    "  plt.plot(hist.history['val_accuracy'],lw=2.0, color='r', label='validation')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.show()\n",
    "  #(model.predict(x) > 0.5).astype(\"int32\")\n",
    "  #np.where(y_pred > threshold, 1,0)\n",
    "  Y_pred =(model.predict(test_seq) > 0.5).astype(\"int32\")# model.predict_class(test_seq)\n",
    "  Y_pred = np.squeeze(Y_pred)\n",
    "  log_result(Y_test,Y_pred,\"fasttext\",\"RNN\",tag)\n",
    "  print(classification_report(Y_test,Y_pred))\n",
    "  #Y_pred = np.squeeze(Y_pred)\n",
    "  utills.confusion_Matrix(Y_test,Y_pred)\n",
    "  utills.PlotRocAuc(Y_test,Y_pred,'blue',\"RNN\")\n",
    "  return model,Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_result(Y_test,Y_pred,feature_name,model_name,tag):\n",
    "    print(\"========= Eperiment - \",tag,\" =========\")\n",
    "    neptune.create_experiment(tag,params=PARAMS)\n",
    "    neptune.append_tag(['DL experiment',feature_name,model_name,tag])\n",
    "    \n",
    "    log_class_metrics(Y_test, Y_pred)\n",
    "    log_confusion_matrix(Y_test, Y_pred)\n",
    "    log_classification_report(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=EMBEDDING_DIM, \n",
    "                    input_dim=LEN_VOCAB, \n",
    "                    input_length=MAX_SEQ_LEN,\n",
    "                    weights=[emb_matrix], # Additionally we give the Wi\n",
    "                    trainable=False)) # Don't train the embeddings - just use GloVe embeddings\n",
    "# We can start with pre-trained embeddings and then fine-tune them using our data by setting trainable to True\n",
    "model.add(SimpleRNN(128, activation='relu',dropout=0.2, recurrent_dropout=0.3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "optimizer_adam = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer_adam,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial,emb_matrix):\n",
    "    layers = 4\n",
    "    units = [8,16,32,64,128,300]\n",
    "    unit = trial.suggest_int(\"units\",units)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=EMBEDDING_DIM, \n",
    "                        input_dim=LEN_VOCAB, \n",
    "                        input_length=MAX_SEQ_LEN,\n",
    "                        weights=[emb_matrix], # Additionally we give the Wi\n",
    "                        trainable=False)) # Don't train the embeddings - just use GloVe embeddings\n",
    "    # We can start with pre-trained embeddings and then fine-tune them using our data by setting trainable to True\n",
    "\n",
    "    model.add(SimpleRNN(unit, activation='relu',dropout=0.2, recurrent_dropout=0.3))\n",
    "    dropout_rate = trial.suggest_float(\"dropout\",0.0,0.2,0.3)\n",
    "    #activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\", \"swish\"])\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(unit, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer_adam = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=optimizer_adam,\n",
    "                metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {'epoch': EPOCHS,\n",
    "          'lr': 0.0001,\n",
    "          'batch':BATCH_SIZE,\n",
    "          'stemmed':False,\n",
    "          'embedding':'fasttext',\n",
    "          'emb_trainable':False\n",
    "          }\n",
    "RNN_model,prediction = train_model(model,X_train,X_test,Y_train,Y_test,\"SimpleRNN + fasttext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import trial\n",
    "def objective(trial):  # trial is object\n",
    "    model = create_model(trial)\n",
    "    scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n",
    "    lr = LearningRateScheduler(scheduler, verbose=0)\n",
    "    \n",
    "        # Fit the model on the training data.\n",
    "        # The TFKerasPruningCallback checks for pruning condition every epoch.\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[TFKerasPruningCallback(trial, \"val_loss\")],\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1,\n",
    "    )\n",
    "    f1_score = model_performence(model)\n",
    "    return f1_score\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective,n_trials=N_TRIALS)  # number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective, n_trials=100)\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "# #         lr = WarmupExponentialDecay(lr_base=1e-3, decay=1e-5, warmup_epochs=30)\n",
    "#         es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "    \n",
    "#         checkpoint_filepath = f\"folds{fold}.hdf5\"\n",
    "#         sv = keras.callbacks.ModelCheckpoint(\n",
    "#             checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "#             save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "#             options=None\n",
    "#         )\n",
    "\n",
    "#         model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n",
    "#         #model.save(f'Fold{fold+1} RNN Weights')\n",
    "#         test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "model.save('my_model')\n",
    "\n",
    "run_2['my_model/saved_model'].upload('my_model/saved_model.pb')\n",
    "\n",
    "for name in glob.glob('my_model/variables/*'):\n",
    "    run_2[name].upload(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log single String\n",
    "run['aux/text'] = 'text I keep track of, like query or tokenized word'\n",
    "\n",
    "# Log series of String to one log\n",
    "for epoch in range(epochs_nr):\n",
    "    token = str(...)\n",
    "    run['train/tokens'].log(token)\n",
    "## CSV file\n",
    "#run['test/preds'].upload('path/to/test_preds.csv')\n",
    "from neptune.new.types import File\n",
    "\n",
    "# Pandas DataFrame \n",
    "#run['data/sample'].upload(File.as_html(sample_df))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
