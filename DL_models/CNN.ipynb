{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "samarasinghe et.al\r\n",
    "\r\n",
    "## CNN architecture [kim yoon (2011)]\r\n",
    "\r\n",
    "The CNN had 4 convolutional layers with each having 256\r\n",
    "convolutional filters with window sizes of 1, 2, 3 and 4 with the activation function as relu. These convolutional filters are able to extract features with 1 to 4 words. Before passing data into the CNN, padding of 0s had to be applied due to the different lengths of the sentences. Each convolutional layer was followed by a pooling layer that uses max pooling as its operator. Subsequently, a dropout layer was applied in order to avoid over fitting. After the convolutional layers, outputs of all dropout layers were concatenated and flattened to be passed into a fully connected neural network. Then, 2 layers with 16 and 1 neurons were used in the fully connected neural network with relu and sigmoid as activation functions, respectively.\r\n",
    "E."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import neptune\r\n",
    "from neptunecontrib.monitoring.metrics import expand_prediction, log_class_metrics, log_binary_classification_metrics, log_classification_report,log_confusion_matrix,log_prediction_distribution\r\n",
    "import os\r\n",
    "from dotenv import load_dotenv\r\n",
    "\r\n",
    "load_dotenv()\r\n",
    "NEPTUNE_PROJECT= os.getenv('NEPTUNE_PROJECT')\r\n",
    "NEPTUNE_API_TOKEN = os.getenv(('NEPTUNE_API_TOKEN'))\r\n",
    "neptune.init(project_qualified_name= NEPTUNE_PROJECT,api_token=NEPTUNE_API_TOKEN) \r\n",
    "             "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Flatten\r\n",
    "from keras.layers.convolutional import Conv1D\r\n",
    "from keras.layers.convolutional import MaxPooling1D\r\n",
    "from keras.layers.embeddings import Embedding\r\n",
    "from keras.preprocessing import sequence\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "import sys\r\n",
    "sys.path.insert(1,'G:\\\\Github\\\\Sinhala-Hate-Speech-Detection')\r\n",
    "import utills"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# load datasets\r\n",
    "path = '../Datasets/processed/no_preprocessing/'\r\n",
    "df_A = pd.read_csv(path+'df_A.csv') \r\n",
    "# df_B = pd.read_csv(path+'df_B.csv')    # fb dataset -kaggle\r\n",
    "# df_C = pd.read_csv(path+'df_C.csv') \r\n",
    "# df_D = pd.read_csv(path+'df_D.csv')  \r\n",
    "# df_all = pd.read_csv(path+'df_all.csv') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Padding the data samples to a maximum review length in words\r\n",
    "max_words = 450\r\n",
    "X_train,X_test,Y_train,Y_test = utills.prepare_dataset(df_A,\"df_A\")\r\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\r\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df_A 6468\n",
      "X train (4527,) Y train (4527,) X test (1941,) Y test (1941,)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'මාත් මේ පුක වලලං මැරෙන්න හදනගමං '",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c1ad427ebff6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmax_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m450\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutills\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_A\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"df_A\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    156\u001b[0m   return sequence.pad_sequences(\n\u001b[0;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m keras_export(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# check `trunc` has expected shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mtrunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'මාත් මේ පුක වලලං මැරෙන්න හදනගමං '"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Building the CNN Model\r\n",
    "model = Sequential()      # initilaizing the Sequential nature for CNN model\r\n",
    "\r\n",
    "# Adding the embedding layer which will take in maximum of 450 words as input and provide a 32 dimensional output of those words which belong in the top_words dictionary\r\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\r\n",
    "\r\n",
    "\r\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\r\n",
    "model.add(MaxPooling1D())\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(250, activation='relu'))\r\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fitting the data onto model\r\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\r\n",
    "\r\n",
    "# Getting score metrics from our model\r\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\r\n",
    "\r\n",
    "# Displays the accuracy of correct sentiment prediction over test data\r\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cnn = Sequential()\r\n",
    "cnn.add(Embedding(input_dim=137, \r\n",
    "                    output_dim=100, \r\n",
    "                    input_length=1223))\r\n",
    "cnn.add(Conv1D(filters=100, \r\n",
    "                 kernel_size=5, \r\n",
    "                 activation='relu'))\r\n",
    "cnn.add(Flatten())\r\n",
    "cnn.add(Dense(100, activation='relu'))\r\n",
    "cnn.add(Dense(1, activation='sigmoid'))cnn.compile(optimizer='adam', \r\n",
    "              loss=\"binary_crossentropy\",\r\n",
    "              metrics=['accuracy'])cnn.fit(X_train, y_train, \r\n",
    "          epochs=10, \r\n",
    "          verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_score = 0.9626\r\n",
    "test_score = cnn.evaluate(X_test, y_test)[1]\r\n",
    "y_pred = cnn.predict_classes(X_test)\r\n",
    "y_pred = y_pred.flatten()_f1_score = f1_score(y_test, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#load embeddings\r\n",
    "print('loading word embeddings...')\r\n",
    "embeddings_index = {}\r\n",
    "f = codecs.open('../Embedding_models/cc.si.300.bin.gz', encoding='utf-8')\r\n",
    "for line in tqdm(f):\r\n",
    "    values = line.rstrip().rsplit(' ')\r\n",
    "    word = values[0]\r\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\r\n",
    "    embeddings_index[word] = coefs\r\n",
    "f.close()\r\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import fasttext\r\n",
    "import fasttext.util\r\n",
    "ft = fasttext.load_model('../Embedding_models/cc.si.300.bin')\r\n",
    "ft.get_dimension()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-53f76ff32cc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Embedding_models/cc.si.300.bin.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\r\n",
    "print(\"tokenizing input data...\")\r\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\r\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\r\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\r\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\r\n",
    "word_index = tokenizer.word_index\r\n",
    "print(\"dictionary size: \", len(word_index))\r\n",
    "\r\n",
    "#pad sequences\r\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\r\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "#training params\r\n",
    "batch_size = 256 \r\n",
    "num_epochs = 8 \r\n",
    "\r\n",
    "#model parameters\r\n",
    "num_filters = 64 \r\n",
    "embed_dim = 300 \r\n",
    "weight_decay = 1e-4\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#embedding matrix\r\n",
    "print('preparing embedding matrix...')\r\n",
    "words_not_found = []\r\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\r\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\r\n",
    "for word, i in word_index.items():\r\n",
    "    if i >= nb_words:\r\n",
    "        continue\r\n",
    "    embedding_vector = embeddings_index.get(word)\r\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\r\n",
    "        # words not found in embedding index will be all-zeros.\r\n",
    "        embedding_matrix[i] = embedding_vector\r\n",
    "    else:\r\n",
    "        words_not_found.append(word)\r\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "#CNN architecture\r\n",
    "print(\"training CNN ...\")\r\n",
    "model = Sequential()\r\n",
    "model.add(Embedding(nb_words, embed_dim,\r\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\r\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\r\n",
    "model.add(MaxPooling1D(2))\r\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\r\n",
    "model.add(GlobalMaxPooling1D())\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\r\n",
    "model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\r\n",
    "\r\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\r\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n",
    "model.summary()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#define callbacks\r\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\r\n",
    "callbacks_list = [early_stopping]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#model training\r\n",
    "hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_test_pred = model.predict(word_seq_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#generate plots\r\n",
    "plt.figure()\r\n",
    "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\r\n",
    "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\r\n",
    "plt.title('CNN sentiment')\r\n",
    "plt.xlabel('Epochs')\r\n",
    "plt.ylabel('Cross-Entropy Loss')\r\n",
    "plt.legend(loc='upper right')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\r\n",
    "plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\r\n",
    "plt.title('CNN sentiment')\r\n",
    "plt.xlabel('Epochs')\r\n",
    "plt.ylabel('Accuracy')\r\n",
    "plt.legend(loc='upper left')\r\n",
    "plt.show()\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## word2vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from gensim.models import word2vec\r\n",
    "#model = Word2Vec.load_word2vec_format(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/cbow_300.w2v\")\r\n",
    "path = \"../Embedding_models/cbow_300.w2v\"\r\n",
    "model = word2vec.Word2Vec.load(path)\r\n",
    "#gensim.models.KeyedVectors.load_word2vec_format\r\n",
    "words = list(model.wv.vocab)\r\n",
    "print(len(words))\r\n",
    "print(words)\r\n",
    "model.most_similar('නීතිය')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e64bd3aaeaa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#model = Word2Vec.load_word2vec_format(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/cbow_300.w2v\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../Embedding_models/cbow_300.w2v\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#gensim.models.KeyedVectors.load_word2vec_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}