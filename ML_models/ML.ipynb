{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Obectives\r\n",
    "\r\n",
    "1. LR, SVM, MNB with Gridsearch selected parameters\r\n",
    "2. Features BOW,TFIDF,w2v\r\n",
    "3. Can easily add new feature or model by creating new function\r\n",
    "\r\n",
    "\r\n",
    "## TODO\r\n",
    "\r\n",
    "1. Bagging model, Boosting model\r\n",
    "2. Word embedding\r\n",
    "3. Gridsearch again for ngrams for char \r\n",
    "4. explore more paremeter for countvectorizer and TFIDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up Neptune for logging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import neptune\r\n",
    "from neptunecontrib.monitoring.metrics import expand_prediction, log_class_metrics, log_binary_classification_metrics, log_classification_report,log_confusion_matrix,log_prediction_distribution\r\n",
    "from neptunecontrib.api import log_table\r\n",
    "import os\r\n",
    "from dotenv import load_dotenv\r\n",
    "\r\n",
    "load_dotenv()\r\n",
    "NEPTUNE_PROJECT= os.getenv('NEPTUNE_PROJECT')\r\n",
    "NEPTUNE_API_TOKEN = os.getenv(('NEPTUNE_API_TOKEN'))\r\n",
    "neptune.init(project_qualified_name= NEPTUNE_PROJECT,api_token=NEPTUNE_API_TOKEN) \r\n",
    "             "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load necessary modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "#from sklearn.metrics import accuracy_score, f1_score, precision_score,roc_curve,roc_auc_score,confusion_matrix,recall_score\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "import re\r\n",
    "import gensim\r\n",
    "\r\n",
    "# import helper function script\r\n",
    "import sys\r\n",
    "sys.path.insert(1,'G:\\\\Github\\\\Sinhala-Hate-Speech-Detection')\r\n",
    "import utills"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# load datasets\r\n",
    "path = '../Datasets/processed/no_preprocessing/'\r\n",
    "df_A = pd.read_csv(path+'df_A.csv')    \r\n",
    "df_B = pd.read_csv(path+'df_B.csv')    # fb dataset -kaggle\r\n",
    "df_C = pd.read_csv(path+'df_C.csv') \r\n",
    "df_D = pd.read_csv(path+'df_D.csv') \r\n",
    "df_A_B = pd.read_csv(path+'df_A_B.csv') \r\n",
    "df_B_C_D = pd.read_csv(path+'df_B_C_D.csv') \r\n",
    "df_A_B_C = pd.read_csv(path+'df_A_B_C.csv') \r\n",
    "df_A_C_D = pd.read_csv(path+'df_A_C_D.csv') \r\n",
    "df_A_B_D = pd.read_csv(path+'df_A_B_D.csv') \r\n",
    "df_all = pd.read_csv(path+'df_all.csv') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "def classifier_feature(datasets,models,features):\r\n",
    "    final_result =pd.DataFrame(columns=['Accuracy','F1-score','Recall','Precision','AUC'])\r\n",
    "    for df_name,df in datasets.items():\r\n",
    "        df_result =pd.DataFrame(columns=['Accuracy','F1-score','Recall','Precision','AUC'])\r\n",
    "        X_train,X_test,Y_train,Y_test = utills.prepare_dataset(df,df_name)\r\n",
    "        for feature_name,feature in features.items():\r\n",
    "            feature_result =pd.DataFrame(columns=['Accuracy','F1-score','Recall','Precision','AUC'])\r\n",
    "            X_train_f,X_test_f = feature(X_train,X_test)\r\n",
    "            for model_name,model in models.items():\r\n",
    "                name = df_name + \"+\" + feature_name+ \"+\"+ model_name\r\n",
    "                print(name)\r\n",
    "                Y_pred = model(X_train_f,X_test_f,Y_train)\r\n",
    "                accuracy, f1_score, recall, precision, auc = result(Y_test,Y_pred)\r\n",
    "                final_result.loc[name] = [accuracy, f1_score, recall, precision, auc]\r\n",
    "                feature_result.loc[model_name] =[accuracy, f1_score, recall, precision, auc]\r\n",
    "                key =model_name + \"+\"+ feature_name\r\n",
    "                df_result.loc[key] = [accuracy, f1_score, recall, precision, auc]\r\n",
    "                log_result(Y_test,Y_pred,name,df_name,feature_name,model_name)\r\n",
    "            print(\" ==== \",feature_name ,\" ==== \")\r\n",
    "            display(feature_result)\r\n",
    "            log_table(feature_name,feature_result)\r\n",
    "        print(\" ==== \",df_name ,\" ==== \")\r\n",
    "        display(df_result)\r\n",
    "        log_table(df_name,df_result)\r\n",
    "    display(final_result)\r\n",
    "    log_table(name,final_result)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# load datasets\r\n",
    "df_dict = {'df_A': df_A,'df_B': df_B,'df_C' :df_C,'df_D' :df_D,'df_A_B' :df_A_B,'df_A_C_D':df_A_C_D,'df_A_B_C':df_A_B_C,'df_A_B_D':df_A_B_D,'df_B_C_D':df_B_C_D,'df_all':df_all}\r\n",
    "# feature dict\r\n",
    "feature_dict={\"bow_word\":bow_word,\"bow_char\":bow_char,\"tfidf_word\":tfidf_word,\"tfidf_char\":tfidf_char}\r\n",
    "# model dict\r\n",
    "model_dict = {\"LR\":lr,\"SVC\":svc,\"MNB\":MNB}\r\n",
    "classifier_feature(df_dict, model_dict, feature_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df_A 6468\n",
      "X train (4527,) Y train (4527,) X test (1941,) Y test (1941,)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df_A+bow_word+LR\n",
      "df_A+bow_word+SVC\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-5fc1983e4530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# model dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"LR\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"SVC\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"MNB\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mMNB\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mclassifier_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-106-7c676e8c057b>\u001b[0m in \u001b[0;36mclassifier_feature\u001b[1;34m(datasets, models, features)\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"+\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"+\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mfinal_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-100-3df0099e881d>\u001b[0m in \u001b[0;36msvc\u001b[1;34m(X_train, X_test, Y_train)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"linear\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    278\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 random_seed)\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\_libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# feature funtions\r\n",
    "\r\n",
    "# bow - word\r\n",
    "def bow_word(X_train,X_test):\r\n",
    "    bow = CountVectorizer(analyzer=\"word\", tokenizer=lambda text: text.split(),ngram_range=(1,2),lowercase=False)\r\n",
    "    bow.fit(X_train)\r\n",
    "    X_train_bow = bow.transform(X_train)\r\n",
    "    X_test_bow = bow.transform(X_test)\r\n",
    "\r\n",
    "    #print(bow.get_feature_names()[:20])\r\n",
    "    #print('The shape is', bow.shape)\r\n",
    "    # postion\r\n",
    "    #print(bow.vocabulary_)\r\n",
    "\r\n",
    "    return X_train_bow,X_test_bow\r\n",
    "\r\n",
    "# bow - char\r\n",
    "def bow_char(X_train,X_test):\r\n",
    "    bow = CountVectorizer(analyzer=\"char\",ngram_range=(2,4),lowercase=False)\r\n",
    "    bow.fit(X_train)\r\n",
    "    X_train_bow = bow.transform(X_train)\r\n",
    "    X_test_bow = bow.transform(X_test)\r\n",
    "    # X_train_bow = bow.fit_transform(X_train)\r\n",
    "    # #X_train_bow = bow.transform(X_train)\r\n",
    "    # X_test_bow = bow.transform(X_test)\r\n",
    "    #print(bow.get_feature_names()[:20])\r\n",
    "    #print('The shape is', tfidf.shape)\r\n",
    "    # postion\r\n",
    "    #print(bow.vocabulary_)\r\n",
    "\r\n",
    "    return X_train_bow,X_test_bow\r\n",
    "\r\n",
    "# TFIDF - word\r\n",
    "def tfidf_word(X_train,X_test):\r\n",
    "    tfidf = TfidfVectorizer(analyzer=\"word\",  tokenizer=lambda text: text.split(),ngram_range=(1,2),lowercase=False)\r\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train)\r\n",
    "    #print(X_train_tfidf.shape)\r\n",
    "    #X_train_tfidf = tfidf.transform(X_train)\r\n",
    "    X_test_tfidf = tfidf.transform(X_test)\r\n",
    "    # print(tfidf.get_feature_names()[:20])\r\n",
    "    # #print('The shape is', tfidf.shape)\r\n",
    "    # # postion\r\n",
    "    # print(tfidf.vocabulary_)\r\n",
    "    return X_train_tfidf,X_test_tfidf\r\n",
    "\r\n",
    "# TFIDF - char\r\n",
    "def tfidf_char(X_train,X_test):\r\n",
    "    tfidf = TfidfVectorizer(analyzer=\"char\",ngram_range=(2,4),lowercase=False)\r\n",
    "    X_train_tfidf  = tfidf.fit_transform(X_train)\r\n",
    "    #X_train_tfidf = tfidf.transform(X_train)\r\n",
    "    X_test_tfidf = tfidf.transform(X_test)\r\n",
    "\r\n",
    "    # print(tfidf.get_feature_names()[:20])\r\n",
    "    # #print('The shape is', tfidf.shape)\r\n",
    "    # # postion\r\n",
    "    # print(tfidf.vocabulary_)\r\n",
    "    \r\n",
    "    return X_train_tfidf,X_test_tfidf\r\n",
    "\r\n",
    "# word2vec\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# fasttext"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "#  model functions\r\n",
    "# lr\r\n",
    "def lr(X_train,X_test,Y_train):\r\n",
    "    lr = LogisticRegression(C=10,max_iter=350)\r\n",
    "    lr.fit(X_train,Y_train)\r\n",
    "    Y_pred = lr.predict(X_test)\r\n",
    "    return Y_pred\r\n",
    "\r\n",
    "\r\n",
    "# SVC\r\n",
    "def svc(X_train,X_test,Y_train):\r\n",
    "    svc = SVC(kernel = \"linear\")\r\n",
    "    svc.fit(X_train,Y_train)\r\n",
    "    Y_pred = svc.predict(X_test)\r\n",
    "    return Y_pred\r\n",
    "\r\n",
    "# MNB\r\n",
    "def MNB(X_train,X_test,Y_train):\r\n",
    "    nb = MultinomialNB(alpha =0.01)\r\n",
    "    nb.fit(X_train,Y_train)\r\n",
    "    Y_pred = nb.predict(X_test)\r\n",
    "    Y_prob = nb.predict_proba(X_test)[:,1]\r\n",
    "    return Y_pred\r\n",
    "\r\n",
    "# XGBoost\r\n",
    "# .fit(X_train,Y_train)\r\n",
    "# Y_pred = .predict(X_test)\r\n",
    "# return Y_pred\r\n",
    "\r\n",
    "\r\n",
    "# Randomfroest"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def dataset_check(df_list,vectorizer,feature_name,model,model_name):\r\n",
    "    \r\n",
    "    final_result =pd.DataFrame(columns=['Accuracy','F1-score','Recall','Precision','AUC'])\r\n",
    "    for name,df in df_list.items():\r\n",
    "        X_train,X_test,Y_train,Y_test = utills.prepare_dataset(df,name)\r\n",
    "        model_pipe, Y_pred = model(X_train,X_test,Y_train,vectorizer,feature_name)\r\n",
    "        # gridsearch function and get best parameter for dataset\r\n",
    "        gridsearch(model_pipe,param_grid,X_train,Y_train)\r\n",
    "        final_result = log_result(Y_test,Y_pred,feature_name,name,final_result,model_name)\r\n",
    "         \r\n",
    "    \r\n",
    "    return final_result\r\n",
    "      \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "def result(y_test, y_pred):\r\n",
    "    import sklearn.metrics as metrics\r\n",
    "    import matplotlib.pyplot as plt\r\n",
    "    import seaborn as sns\r\n",
    "    import numpy as np\r\n",
    "\r\n",
    "    classes = np.unique(y_test)\r\n",
    "    #y_test_array = pd.get_dummies(y_test, drop_first=False).values\r\n",
    "    ## Accuracy, Precision, Recall\r\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\r\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\r\n",
    "    precision = metrics.precision_score(y_test, y_pred)\r\n",
    "    recall = metrics.recall_score(y_test, y_pred)\r\n",
    "    f1_score = metrics.f1_score(y_test, y_pred)\r\n",
    "#     print(\"Accuracy:\",  round(accuracy, 2))\r\n",
    "#     print(\"Auc:\", round(auc, 2))\r\n",
    "#     print(\"Precsion:\", round(precision, 2))\r\n",
    "#     print(\"f1_score:\", round(f1_score, 2))\r\n",
    "#     print(\"recall:\", round(recall, 2))\r\n",
    "#     print(\"Detail:\")\r\n",
    "    #print(metrics.classification_report(y_test, y_pred))\r\n",
    "\r\n",
    "    # Plot confusion matrix\r\n",
    "#     cm = metrics.confusion_matrix(y_test, y_pred)\r\n",
    "#     fig, ax = plt.subplots()\r\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\r\n",
    "#     ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", xticklabels=classes,\r\n",
    "#            yticklabels=classes, title=\"Confusion matrix\")\r\n",
    "#     plt.yticks(rotation=0)\r\n",
    "#     plt.show()\r\n",
    "#     utills.PlotRocAuc(y_test, y_pred, 'green', 'LR')\r\n",
    "  \r\n",
    "    return (accuracy, f1_score, recall, precision, auc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def log_result(Y_test,Y_pred,name,df_name,feature_name,model_name):\r\n",
    "    \r\n",
    "    \r\n",
    "    print(\"========= Eperiment - \",name,\" =========\")\r\n",
    "    neptune.create_experiment(name)\r\n",
    "    neptune.append_tag(['Dataset experiment 3',df_name,feature_name,model_name,name])\r\n",
    "    \r\n",
    "    log_class_metrics(Y_test, Y_pred)\r\n",
    "    log_confusion_matrix(Y_test, Y_pred)\r\n",
    "    log_classification_report(Y_test, Y_pred)\r\n",
    "\r\n",
    "    #accuracy,f1_score,recall,precision,auc = result(Y_test,Y_pred)\r\n",
    "    \r\n",
    "    # neptune.log_metric('acc', accuracy)\r\n",
    "    # neptune.log_metric('auc', auc)\r\n",
    "    # neptune.log_metric('recall', recall)\r\n",
    "    # neptune.log_metric('precison', precision)\r\n",
    "    # neptune.log_metric('f1_score', f1_score)\r\n",
    "    \r\n",
    "    \r\n",
    "    #final_result.loc[df_name] = [accuracy,f1_score,recall,precision,auc]\r\n",
    "    \r\n",
    "    #return final_result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loading pretrained FastText word embeddings in Sinhala\r\n",
    "import fasttext\r\n",
    "import fasttext.util\r\n",
    "ft = fasttext.load_model('/content/cc.si.300.bin')\r\n",
    "ft.get_dimension()\r\n",
    "# Mapping FastText word vectors with word in the dataset \r\n",
    "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\r\n",
    "for word, i in word_index.items():\r\n",
    "    embedding_vector = ft.get_word_vector(word)\r\n",
    "    if embedding_vector is not None:\r\n",
    "        embeddings_matrix[i] = embedding_vector"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}