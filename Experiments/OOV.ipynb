{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find OOV count of word2vec CBOW, skip gram and fasttext of common crawl datasets \n",
    "    1. without stemming \n",
    "    2. stemming whole dataset\n",
    "    3. only stemming already OOV words in train dataset\n",
    "\n",
    " This experiment will be done only to train dataset\n",
    "\n",
    " #### Results \n",
    " Vocabluary coverage\n",
    "\n",
    " | embedding | without stemming | stemming whole dataset | only stemming OOV words |\n",
    " | --- | --- | --- | --- |\n",
    " | w2v cbow  | 92.84%  1584| 83.29% 2784 | 96.17%  846 | \n",
    " | w2v sgram  | 92.84%  1584 | 83.29%  2784| 96.17% 846 |\n",
    " | fasttext  FB |100% | 100%| |\n",
    "\n",
    " Dataset coverage\n",
    "\n",
    "  | embedding | without stemming | stemming whole dataset | only stemming OOV words |\n",
    " | --- | --- | --- | --- |\n",
    " | w2v cbow  | 98.11% | 95.93%| 99% |\n",
    " | w2v sgram  | 98.11% | 95.93%| 99%  |\n",
    " | fasttext  FB | 100% | 100% | |\n",
    "\n",
    "1. When use with embedding its best to not stemm dataset and do the improvemnt\n",
    "2. expected improvement was  achived by only stemming OOV words with shorter suffix and longer sufiix combintion around 4% increasement in coverage\n",
    "3. When handling OOV words assingg random vector form same doistribution for words with freq > 1 and for others assing mean embedding vector or 0\n",
    "4. Leveraged fasttext rare word n-gram techniqe to get embeddings for non existed words in fasttetx vacab\n",
    " #### Further Experiments \n",
    "    1. Select on embedidng and evaluate performence of each different above processed dataset on DL model\n",
    "    2. Evaluate on smaarasinghe et. al CNN as already done second method (stemming whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import accuracy_score, f1_score, precision_score,roc_curve,roc_auc_score,confusion_matrix,recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import gensim\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "# import helper function script\n",
    "import sys\n",
    "sys.path.insert(1,'G:\\\\Github\\\\Sinhala-Hate-Speech-Detection')\n",
    "import utills\n",
    "import sinhala_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets - no preprocessing\n",
    "path = '../Datasets/processed/no_preprocessing/'\n",
    "df_A = pd.read_csv(path+'df_A.csv')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "මේ වේසිට නීතිය ක්‍රියාත්මක වෙන්නෙ කවදාද ?\n"
     ]
    }
   ],
   "source": [
    "out = utills.removeExtraSpaces(df_A['comment'][0])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess everything except stemming and save dataset\n",
    "df_A_p = utills.preprocess(df_A,'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_p.to_csv(\"../Datasets/processed/no_stemming/df_A.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets stemmed and preprocessed \n",
    "path = '../Datasets/processed/preprocessed/'\n",
    "df_A_s = pd.read_csv(path+'df_A.csv')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>df</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>&lt;&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>&lt;3,</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     comment  label df cleaned\n",
       "610       <>      0  A        \n",
       "3070     <3,      0  A        "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_A_p[df_A_p['cleaned']==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_p.drop(index =[610,3070],inplace=True)\n",
    "df_A_s.drop(index =[610,3070],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_not_found = []\n",
    "def word_avg_vector(model,words_list):\n",
    "    if len(words_list) < 1:  # whole sentence has no words or nan\n",
    "        return np.zeros(300)\n",
    "    else:\n",
    "        vectorized = []\n",
    "        for word in words_list:\n",
    "          if word in model:\n",
    "            #print(\"word vector length\",len(model[word]))\n",
    "            vectorized.append(model[word])\n",
    "          else:\n",
    "            vectorized.append(np.zeros(300))\n",
    "            words_not_found.append(word)\n",
    "        #vectorized = [model[word] if word in model else np.zeros(300) for word in words_list]\n",
    "        #print(vectorized)\n",
    "        #print(\"vectorized array len \",len(vectorized),np.array(vectorized).shape)\n",
    "    # doc = [word for word in doc if word in model.wv.index_to_key else np.random.rand(k)]\n",
    "    #print('average embedding shape',np.mean(vectorized, axis=0).shape)\n",
    "    return np.mean(vectorized, axis=0)\n",
    "\n",
    "def get_embedding(df,model):\n",
    "    # TODO add option to select TFIDF vs mean embedding\n",
    "    # avg vector\n",
    "    embeddings = df.apply(lambda x: word_avg_vector(model,x.split()))\n",
    "\n",
    "    # tfidf weighted vector\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as FastText\n",
    "from gensim.test.utils import datapath\n",
    "model_fasttext = FastText.fasttext.load_facebook_model(datapath(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/cc.si.300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"internal executive\" in model_fasttext.wv.key_to_index)\n",
    "print(\"internal executive\" in model_fasttext.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=818830, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.models.fasttext as fasttext\n",
    "# fasttext_model = fasttext.FastText.load(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/fasttext_uom/fasttext_300.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_model(emb_model):\n",
    "    from gensim.models import word2vec\n",
    "    import gensim.models as FastText\n",
    "    from gensim.test.utils import datapath\n",
    "    if emb_model == \"w2v_skipgram\":\n",
    "        model = word2vec.Word2Vec.load(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/word2vec/word2vec_300.w2v\")\n",
    "    elif emb_model == \"w2v_cbow\":\n",
    "        model = word2vec.Word2Vec.load(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/CBOW-word2vec/cbow_300.w2v\")\n",
    "    elif emb_model == \"fasttext\":\n",
    "        #FastText.load_fasttext_format(\"../../../corpus/analyzed/saved_models/wiki.si.bin\")\n",
    "        model = FastText.fasttext.load_facebook_model(datapath(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/cc.si.300.bin\"))\n",
    "        #model = word2vec.Word2Vec.load(\"G:/Github/Sinhala-Hate-Speech-Detection/Embedding_models/fasttext_300.w2v\")\n",
    "    else:\n",
    "        print(\"Invalid argument. Need w2v_skipgram or w2v_cbow or fasttext as argument\")\n",
    "        model = None\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_index(model):\n",
    "    from gensim.models import word2vec\n",
    "    embeddings_index ={}\n",
    "    for index, word in enumerate(model.wv.index_to_key):\n",
    "        embeddings_index[word] = model.wv.get_vector(word)\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# embeddings_index = {}\n",
    "# for i, line in enumerate(open('/content/drive/My Drive/Research Project/Notebooks/cc.si.300.vec')):\n",
    "#     values = line.split()\n",
    "#     embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "# print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    import operator\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_p = build_vocab(df_A_p['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'මේ': 1153, 'වේසිට': 3, 'නීතිය': 53, 'ක්\\u200dරියාත්මක': 39, 'වෙන්නෙ': 109}\n"
     ]
    }
   ],
   "source": [
    "print({k: vocab_p[k] for k in list(vocab_p)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec CBOW : \n",
      "found 2137729 word vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"word2vec CBOW : \")\n",
    "model_cbow = load_emb_model(\"w2v_cbow\")\n",
    "#embed_cbow = get_emb_index(model_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2137729 word vectors\n"
     ]
    }
   ],
   "source": [
    "embed_cbow = get_emb_index(model_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 92.84% of vocab\n",
      "Found embeddings for  98.11% of all text\n",
      "Number of unkown words in cbow =  1584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්පෝ', 29),\n",
       " ('බන්දුලයො', 23),\n",
       " ('ගොසිපෝ', 20),\n",
       " ('ගොසිප්පා', 16),\n",
       " ('කෙනෙහිලිකන්', 14),\n",
       " ('ලොල්ලෙක්ට', 9),\n",
       " ('අංගජාතයක්', 8),\n",
       " ('නැකපතියා', 7),\n",
       " ('ලොල්ලගෙ', 6),\n",
       " ('හදනගමං', 5),\n",
       " ('වලලං', 5),\n",
       " ('අප්පුහාමියා', 5),\n",
       " ('ලක්මිණයා', 4),\n",
       " ('තෙවා', 4),\n",
       " ('ගවයෙක්ද', 4),\n",
       " ('හැදිගෑවිලම', 4),\n",
       " ('නැකපති', 4),\n",
       " ('නැකපතියො', 4),\n",
       " ('ගෙවලයන්ට', 4),\n",
       " ('ඩෙංගුදැයි', 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_cbow = check_coverage(vocab_p, embed_cbow)\n",
    "print(\"Number of unkown words in cbow = \", len(oov_cbow))\n",
    "oov_cbow[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"word2vec skip gram : \")\n",
    "model_skipgram = load_emb_model(\"w2v_skipgram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2137729 word vectors\n",
      "Found embeddings for 92.84% of vocab\n",
      "Found embeddings for  98.11% of all text\n",
      "Number of unkown words in cbow =  1584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්පෝ', 29),\n",
       " ('බන්දුලයො', 23),\n",
       " ('ගොසිපෝ', 20),\n",
       " ('ගොසිප්පා', 16),\n",
       " ('කෙනෙහිලිකන්', 14),\n",
       " ('ලොල්ලෙක්ට', 9),\n",
       " ('අංගජාතයක්', 8),\n",
       " ('නැකපතියා', 7),\n",
       " ('ලොල්ලගෙ', 6),\n",
       " ('හදනගමං', 5),\n",
       " ('වලලං', 5),\n",
       " ('අප්පුහාමියා', 5),\n",
       " ('ලක්මිණයා', 4),\n",
       " ('තෙවා', 4),\n",
       " ('ගවයෙක්ද', 4),\n",
       " ('හැදිගෑවිලම', 4),\n",
       " ('නැකපති', 4),\n",
       " ('නැකපතියො', 4),\n",
       " ('ගෙවලයන්ට', 4),\n",
       " ('ඩෙංගුදැයි', 4)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_skipgram = get_emb_index(model_skipgram)\n",
    "oov_skipgram = check_coverage(vocab_p, embed_skipgram)\n",
    "print(\"Number of unkown words in cbow = \", len(oov_skipgram))\n",
    "oov_skipgram[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්වල', 0.8204402327537537),\n",
       " ('ගොසිප්දේශීය', 0.7919450402259827),\n",
       " ('ගොසිප්99', 0.7867631912231445),\n",
       " ('AMගොසිප්', 0.7855252623558044),\n",
       " ('හිරුගොසිප්', 0.7843061089515686),\n",
       " ('ගොසිප්ප්\\u200dරධාන', 0.7780765295028687),\n",
       " ('ගොසිප', 0.7683893442153931),\n",
       " ('ගොසිප්9', 0.7672510147094727),\n",
       " ('ගොසිප්දේශපාලනයප්\\u200dරධාන', 0.738722562789917),\n",
       " ('PMගොසිප්', 0.724074125289917)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fasttext.wv.most_similar('ගොසිප්පෝ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ගොසිප්පෝ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-c41efa798bdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membed_fasttext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ගොසිප්පෝ'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'ගොසිප්පෝ'"
     ]
    }
   ],
   "source": [
    "embed_fasttext['ගොසිප්පෝ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_OOV(model,oov_vocab,embed_index):\n",
    "    count = 0\n",
    "    for word, freq in oov_vocab:\n",
    "        if word in model.wv:\n",
    "            embed_index[word] = model.wv.get_vector(word)\n",
    "            count+=1\n",
    "    print(f\"Added {count} words to embedding with rare words handling of fasttext\")\n",
    "    return embed_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 818830 word vectors\n",
      "Found embeddings for 86.04% of vocab\n",
      "Found embeddings for  96.35% of all text\n",
      "Number of unkown words in fasttext =  3088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්පෝ', 29),\n",
       " ('බන්දුලයො', 23),\n",
       " ('ගොසිපෝ', 20),\n",
       " ('ගොසිප්පා', 16),\n",
       " ('ගොසිපා', 16),\n",
       " ('කෙනෙහිලිකන්', 14),\n",
       " ('️', 12),\n",
       " ('ලොල්ලෙක්ට', 9),\n",
       " ('අංගජාතයක්', 8),\n",
       " ('ලොල්ලට', 8),\n",
       " ('පොන්සියො', 7),\n",
       " ('නැකපතියා', 7),\n",
       " ('ගවයෙක්ට', 7),\n",
       " ('ගවපාලනේ', 6),\n",
       " ('හැමිනෙන්නේ', 6),\n",
       " ('ලොල්ලගෙ', 6),\n",
       " ('මකබෑවිල', 5),\n",
       " ('බන්දුලයට', 5),\n",
       " ('හරකෙක්නෙ', 5),\n",
       " ('පර්සිව', 5)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_fasttext = get_emb_index(model_fasttext)\n",
    "oov_fasttext = check_coverage(vocab_p, embed_fasttext)\n",
    "print(\"Number of unkown words in fasttext = \", len(oov_fasttext))\n",
    "oov_fasttext[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3088 words to embedding with rare words handling of fasttext\n",
      "Found embeddings for 100.00% of vocab\n",
      "Found embeddings for  100.00% of all text\n",
      "Number of unkown words in fasttext =  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## after handling rare words\n",
    "embed_fasttext_rare =fasttext_OOV(model_fasttext,oov_fasttext,embed_fasttext)\n",
    "oov_fasttext_rare = check_coverage(vocab_p, embed_fasttext_rare)\n",
    "print(\"Number of unkown words in fasttext = \", len(oov_fasttext_rare))\n",
    "oov_fasttext_rare[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With stemmed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_s = build_vocab(df_A_s['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'මේ': 1179, 'වේසිට': 3, 'නීතිය': 53, 'ක්\\u200dරියාත්': 39, 'වෙ': 179}\n"
     ]
    }
   ],
   "source": [
    "print({k: vocab_s[k] for k in list(vocab_s)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec CBOW : \n",
      "Found embeddings for 83.29% of vocab\n",
      "Found embeddings for  95.93% of all text\n",
      "Number of unkown words in cbow =  2784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්ප', 51),\n",
       " ('කාලකණ්', 40),\n",
       " ('මැදමුල', 25),\n",
       " ('හික්ස', 20),\n",
       " ('දැම්මන', 18),\n",
       " ('කරනකො', 16),\n",
       " ('කෙනෙහිලික', 15),\n",
       " ('ජනාදිප', 15),\n",
       " ('මැහ', 14),\n",
       " ('ෆේස්බු', 14),\n",
       " ('පේනවන', 13),\n",
       " ('කලාන', 13),\n",
       " ('හදාග', 13),\n",
       " ('ගොම්ම', 11),\n",
       " ('නැකපති', 11),\n",
       " ('නිවන්ස', 11),\n",
       " ('පුන්නක්', 11),\n",
       " ('කරවෙ', 10),\n",
       " ('හැංග', 10),\n",
       " ('පෙට්\\u200dරල', 10)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word2vec CBOW : \")\n",
    "oov_cbow_s = check_coverage(vocab_s, embed_cbow)\n",
    "print(\"Number of unkown words in cbow = \", len(oov_cbow_s))\n",
    "oov_cbow_s[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec skip gram : \n",
      "Found embeddings for 83.29% of vocab\n",
      "Found embeddings for  95.93% of all text\n",
      "Number of unkown words in cbow =  2784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්ප', 51),\n",
       " ('කාලකණ්', 40),\n",
       " ('මැදමුල', 25),\n",
       " ('හික්ස', 20),\n",
       " ('දැම්මන', 18),\n",
       " ('කරනකො', 16),\n",
       " ('කෙනෙහිලික', 15),\n",
       " ('ජනාදිප', 15),\n",
       " ('මැහ', 14),\n",
       " ('ෆේස්බු', 14),\n",
       " ('පේනවන', 13),\n",
       " ('කලාන', 13),\n",
       " ('හදාග', 13),\n",
       " ('ගොම්ම', 11),\n",
       " ('නැකපති', 11),\n",
       " ('නිවන්ස', 11),\n",
       " ('පුන්නක්', 11),\n",
       " ('කරවෙ', 10),\n",
       " ('හැංග', 10),\n",
       " ('පෙට්\\u200dරල', 10)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word2vec skip gram : \")\n",
    "oov_skipgram_s = check_coverage(vocab_s, embed_skipgram)\n",
    "print(\"Number of unkown words in cbow = \", len(oov_skipgram_s))\n",
    "oov_skipgram_s[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 818830 word vectors\n",
      "Found embeddings for 74.57% of vocab\n",
      "Found embeddings for  92.86% of all text\n",
      "Number of unkown words in fasttext =  4237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('දඩුව', 52),\n",
       " ('ගොසිප්ප', 51),\n",
       " ('මුස්ලි', 44),\n",
       " ('කාලකණ්', 40),\n",
       " ('කාලකන්', 38),\n",
       " ('මැරෙ', 33),\n",
       " ('තේරෙ', 26),\n",
       " ('මැදමුල', 25),\n",
       " ('දුප්ප', 24),\n",
       " ('බන්දුලය', 23),\n",
       " ('බිස්නස', 22),\n",
       " ('කවුර', 22),\n",
       " ('ඉන්නවන', 21),\n",
       " ('හිතාග', 20),\n",
       " ('හික්ස', 20),\n",
       " ('පරිස්ස', 19),\n",
       " ('වැරැද්', 19),\n",
       " ('මොකු', 18),\n",
       " ('දැම්මන', 18),\n",
       " ('ඉස්සෙ', 17)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_fasttext = get_emb_index(model_fasttext)\n",
    "oov_fasttext_s = check_coverage(vocab_s, embed_fasttext)\n",
    "print(\"Number of unkown words in fasttext = \", len(oov_fasttext_s))\n",
    "oov_fasttext_s[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 4237 words to embedding with rare words handling of fasttext\n",
      "Found embeddings for 100.00% of vocab\n",
      "Found embeddings for  100.00% of all text\n",
      "Number of unkown words in fasttext =  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_fasttext_s_rare =fasttext_OOV(model_fasttext,oov_fasttext_s,embed_fasttext)\n",
    "oov_fasttext_s_rare = check_coverage(vocab_s, embed_fasttext_s_rare)\n",
    "print(\"Number of unkown words in fasttext = \", len(oov_fasttext_s_rare))\n",
    "oov_fasttext_s_rare[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only on OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_stem(embedding, vocab):\n",
    "    \n",
    "    import sinhala_stemmer\n",
    "    word_len = 1\n",
    "    stemmer = sinhala_stemmer.SinhalaStemmer()\n",
    "    stem_count = [0,0]\n",
    "    word_list = {}\n",
    "\n",
    "    for word,freq in vocab:\n",
    "        word_ls = stemmer.stem(word, True, word_len)[0]   \n",
    "        word_ss = stemmer.stem(word, False, word_len)[0]  \n",
    "\n",
    "        # longer suffix     \n",
    "        if word not in embedding and word_ls  in embedding:\n",
    "            embedding[word] = embedding[word_ls]\n",
    "            stem_count[0]+=1\n",
    "            word_list[word]=[word_ss,word_ls] \n",
    "\n",
    "        # shorter suffix\n",
    "        elif word not in embedding and word_ss in embedding: \n",
    "            embedding[word] = embedding[word_ss]\n",
    "            stem_count[1]+=1 \n",
    "            word_list[word]=[word_ls,word_ss]  \n",
    "    print(f\"Added {stem_count[0]} words to embedding with longer suffix\")\n",
    "    print(f\"Added {stem_count[1]} words to embedding with shorter suffix\")\n",
    "    print(len(embedding))  \n",
    "    #print(word_list)  \n",
    "    return embedding      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1434\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "list_unk =[]\n",
    "for word,freq in oov_cbow:\n",
    "    if freq < 2:\n",
    "        count+=1\n",
    "        list_unk.append(word)\n",
    "print(len(list_unk))\n",
    "#print(list_unk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2137729 word vectors\n",
      "Added 699 words to embedding with longer suffix\n",
      "Added 39 words to embedding with shorter suffix\n",
      "2138467\n",
      "Found embeddings for 96.17% of vocab\n",
      "Found embeddings for  99.00% of all text\n",
      "Number of unkown words in cbow =  846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්පෝ', 29),\n",
       " ('ගොසිප්පා', 16),\n",
       " ('කෙනෙහිලිකන්', 14),\n",
       " ('නැකපතියා', 7),\n",
       " ('හදනගමං', 5),\n",
       " ('වලලං', 5),\n",
       " ('නැකපති', 4),\n",
       " ('නැකපතියො', 4),\n",
       " ('ඩෙංගුදැයි', 4),\n",
       " ('ලොල්ලෙක්නෙ', 4),\n",
       " ('පික්ස්සු', 3),\n",
       " ('ලොල්ලනෙ', 3),\n",
       " ('නැගිටහල්ලා', 3),\n",
       " ('කුණුහර්පවලින්නෙ', 3),\n",
       " ('ප්\\u200dරද්ශවාසී', 3),\n",
       " ('වහල්ලුනෙ', 3),\n",
       " ('ප්\\u200dරොෙජට්', 3),\n",
       " ('කිරීඩකයෝ', 2),\n",
       " ('අභයාරාමයෙදි', 2),\n",
       " ('ද්ව්යන්ගනවක්', 2)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word length 1\n",
    "embed_cbow = get_emb_index(model_cbow)\n",
    "embed_cbow_stem_1 = add_stem(embed_cbow,oov_cbow)\n",
    "oov_cbow_1 = check_coverage(vocab_p, embed_cbow_stem_1)\n",
    "print(\"Number of unkown words in cbow = \", len(oov_cbow_1))\n",
    "oov_cbow_1[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2137729 word vectors\n",
      "Added 598 words to embedding with longer suffix\n",
      "Added 38 words to embedding with shorter suffix\n",
      "2138365\n",
      "Found embeddings for 95.71% of vocab\n",
      "Found embeddings for  98.88% of all text\n",
      "Number of unkown words in cbow =  948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්පෝ', 29),\n",
       " ('ගොසිප්පා', 16),\n",
       " ('කෙනෙහිලිකන්', 14),\n",
       " ('නැකපතියා', 7),\n",
       " ('හදනගමං', 5),\n",
       " ('වලලං', 5),\n",
       " ('තෙවා', 4),\n",
       " ('නැකපති', 4),\n",
       " ('නැකපතියො', 4),\n",
       " ('ඩෙංගුදැයි', 4),\n",
       " ('ලොල්ලෙක්නෙ', 4),\n",
       " ('පික්ස්සු', 3),\n",
       " ('ලොල්ලනෙ', 3),\n",
       " ('නැගිටහල්ලා', 3),\n",
       " ('කුණුහර්පවලින්නෙ', 3),\n",
       " ('ප්\\u200dරද්ශවාසී', 3),\n",
       " ('වහල්ලුනෙ', 3),\n",
       " ('ප්\\u200dරොෙජට්', 3),\n",
       " ('ලෝගා', 3),\n",
       " ('හම්සව', 3)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word length 5\n",
    "embed_cbow = get_emb_index(model_cbow)\n",
    "embed_cbow_stem_5 = add_stem(embed_cbow,oov_cbow)\n",
    "oov_cbow_5 = check_coverage(vocab_p, embed_cbow_stem_5)\n",
    "print(\"Number of unkown words in cbow = \", len(oov_cbow_5))\n",
    "oov_cbow_5[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2137729 word vectors\n",
      "Added 692 words to embedding with longer suffix\n",
      "Added 39 words to embedding with shorter suffix\n",
      "2138460\n",
      "Found embeddings for 96.14% of vocab\n",
      "Found embeddings for  98.99% of all text\n",
      "Number of unkown words in cbow =  853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ගොසිප්පෝ', 29),\n",
       " ('ගොසිප්පා', 16),\n",
       " ('කෙනෙහිලිකන්', 14),\n",
       " ('නැකපතියා', 7),\n",
       " ('හදනගමං', 5),\n",
       " ('වලලං', 5),\n",
       " ('නැකපති', 4),\n",
       " ('නැකපතියො', 4),\n",
       " ('ඩෙංගුදැයි', 4),\n",
       " ('ලොල්ලෙක්නෙ', 4),\n",
       " ('පික්ස්සු', 3),\n",
       " ('ලොල්ලනෙ', 3),\n",
       " ('නැගිටහල්ලා', 3),\n",
       " ('කුණුහර්පවලින්නෙ', 3),\n",
       " ('ප්\\u200dරද්ශවාසී', 3),\n",
       " ('වහල්ලුනෙ', 3),\n",
       " ('ප්\\u200dරොෙජට්', 3),\n",
       " ('කිරීඩකයෝ', 2),\n",
       " ('අභයාරාමයෙදි', 2),\n",
       " ('ද්ව්යන්ගනවක්', 2)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word length 3\n",
    "embed_cbow = get_emb_index(model_cbow)\n",
    "embed_cbow_stem_3 = add_stem(embed_cbow,oov_cbow)\n",
    "oov_cbow_3 = check_coverage(vocab_p, embed_cbow_stem_3)\n",
    "print(\"Number of unkown words in cbow = \", len(oov_cbow_3))\n",
    "oov_cbow_3[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further evaluation on DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(X):\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(X)\n",
    "    X = t.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=max_len)\n",
    "    return X, t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, word_index = make_data(train['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embed_matrix(embeddings_index, word_index, len_voc):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding = make_embed_matrix(embed_glove, word_index, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n",
    "    inp    = Input(shape=(max_len,))\n",
    "    x      = Embedding(len_voc, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x      = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x      = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    avg_pl = GlobalAveragePooling1D()(x)\n",
    "    max_pl = GlobalMaxPooling1D()(x)\n",
    "    concat = concatenate([avg_pl, max_pl])\n",
    "    dense  = Dense(64, activation=\"relu\")(concat)\n",
    "    drop   = Dropout(0.1)(concat)\n",
    "    output = Dense(1, activation=\"sigmoid\")(concat)\n",
    "    \n",
    "    model  = Model(inputs=inp, outputs=output)\n",
    "    model.compile(loss=loss, optimizer=Adam(lr=0.0001), metrics=['accuracy', f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint('weights.hdf5', monitor=\"val_f1\", mode=\"max\", verbose=True, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1', factor=0.1, patience=2, verbose=1, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = model.predict(X_val, batch_size=512, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
