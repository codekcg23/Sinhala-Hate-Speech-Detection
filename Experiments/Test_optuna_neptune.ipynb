{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogQ-wIb5KhYH"
   },
   "source": [
    "### Experiment : Set up optuna with neptune for Hyper parameter and architecture buildinng\n",
    "### Objectives\n",
    "https://www.kaggle.com/mistag/keras-model-tuning-with-optuna\n",
    "1. batch size =[8,16,32]\n",
    "2. learining rate = [0.1,0.01,0.0001,0.00001]\n",
    "3. units = [8,16,32,64,128]\n",
    "4. number of layers in architecture [4,5,6]\n",
    "5. optimization = [adam,]\n",
    "6. epochs = [10,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ikmcc1WMKgNK"
   },
   "source": [
    "### Observations\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF8BkDzFKg9f"
   },
   "source": [
    "### Solutions and further investigation\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical parameters – uses the trials.suggest_categorical() method. You need to provide the name of the parameter and its choices.\n",
    "\n",
    "Integer parameters – uses the trials.suggest_int() method. You need to provide the name of the parameter, low and high value.\n",
    "\n",
    "Float parameters – uses the trials.suggest_float() method. You need to provide the name of the parameter, low and high value.\n",
    "\n",
    "Continuous parameters – uses the trials.suggest_uniform() method. You need to provide the name of the parameter, low and high value.\n",
    "\n",
    "Discrete parameters – uses the trials.suggest_discrete_uniform() method. You need to provide the name of the parameter, low value, high value, and step of discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### samplers\n",
    "    GridSampler – It uses a grid search. The trials suggest all combinations of parameters in the given search space during the study.\n",
    "\n",
    "    RandomSampler – It uses random sampling. This sampler is based on independent sampling.\n",
    "\n",
    "    TPESampler – It uses the TPE (Tree-structured Parzen Estimator) algorithm.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2xtbDAWLGVd"
   },
   "source": [
    "### Load Modeules and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rn\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "# from keras import layers\n",
    "from sklearn import metrics\n",
    "# from keras import regularizers\n",
    "# from keras import models,optimizers\n",
    "# from keras.models import Sequential\n",
    "# from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "# from keras.layers import Dense,Input,SpatialDropout1D,Dropout,Flatten, SimpleRNN,LSTM,RNN,GRU\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "# import helper function script\n",
    "import sys\n",
    "sys.path.insert(1,'G:\\\\Github\\\\Sinhala-Hate-Speech-Detection')\n",
    "import utills\n",
    "import sinhala_stemmer\n",
    "import TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There is a new version of neptune-client 0.14.2 (installed: 0.10.2).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Project(codekcg23/Research-Experiments)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import neptune\n",
    "#import neptune.new as neptune\n",
    "from neptunecontrib.monitoring.metrics import expand_prediction, log_class_metrics, log_binary_classification_metrics, log_classification_report,log_confusion_matrix,log_prediction_distribution\n",
    "from neptunecontrib.api import log_table\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "import neptunecontrib.monitoring.optuna as opt_utils \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "NEPTUNE_PROJECT= os.getenv('NEPTUNE_PROJECT')\n",
    "NEPTUNE_API_TOKEN = os.getenv(('NEPTUNE_API_TOKEN'))\n",
    "neptune.init(project_qualified_name= NEPTUNE_PROJECT,api_token=NEPTUNE_API_TOKEN) \n",
    "\n",
    "# import neptune.new.integrations.optuna as optuna_utils\n",
    "\n",
    "# neptune_callback = optuna_utils.NeptuneCallback(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "\n",
      "X train (4526,) Y train (4526,) X test (1940,) Y test (1940,)\n",
      "dictionary size:  17577\n"
     ]
    }
   ],
   "source": [
    "TC = TextClassifier.TextClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial, TC,model_type=None):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.optimizer_v2.adam import Adam\n",
    "    from tensorflow.keras.layers import Dense,Input,SpatialDropout1D,Dropout,Flatten, SimpleRNN,LSTM,RNN,GRU,Bidirectional\n",
    "    from tensorflow.keras.layers.embeddings import Embedding\n",
    "    from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "    leaky_relu = LeakyReLU(alpha=0.01)\n",
    "\n",
    "    layer = trial.suggest_categorical(\"layers\",[0,1,2,3])\n",
    "    unit = trial.suggest_categorical(\"units\",[8,16,32,64,128])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rates\",[0.0, 0.4, 0.1])\n",
    "    recurr_dropout = trial.suggest_float(\"recurrent_dropout_rates\",[0.0, 0.3, 0.1])\n",
    "    activ = trial.suggest_categorical(\"activation\", [\"relu\",  leaky_relu, \"elu\",\"tanh\"])\n",
    "\n",
    "    print(\"Build ML model\")\n",
    "    model = Sequential()\n",
    "    if(TC.EMBEDDING == None):\n",
    "        model.add(Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
    "                        input_dim=TC.LEN_VOCAB, \n",
    "                        input_length=TC.MAX_SEQ_LEN,\n",
    "                       # weights=[TC.emb_matrix], # Additionally we give the Wi\n",
    "                        trainable=TC.trainable))\n",
    "\n",
    "    else:\n",
    "        model.add(Embedding(output_dim=TC.EMBEDDING_SIZE, \n",
    "                        input_dim=TC.LEN_VOCAB, \n",
    "                        input_length=TC.MAX_SEQ_LEN,\n",
    "                        weights=[TC.emb_matrix], # Additionally we give the Wi\n",
    "                        trainable=TC.trainable)) # Don't train the embeddings - just use GloVe embeddings\n",
    "    # We can start with pre-trained embeddings and then fine-tune them using our data by setting trainable to True\n",
    "    if(model_type==\"RNN\"):\n",
    "        if layer!=0:\n",
    "            for i in range(layer):\n",
    "                model.add(SimpleRNN(unit, activation=activ,dropout=dropout_rate, recurrent_dropout=recurr_dropout,return_sequences=True))\n",
    "        model.add(SimpleRNN(unit, activation=activ,dropout=dropout_rate, recurrent_dropout=recurr_dropout))\n",
    "\n",
    "    elif(model_type == \"LSTM\"):\n",
    "        if layer!=0:\n",
    "            for i in range(layer):\n",
    "                model.add(LSTM(unit, activation=activ,dropout_rate, recurrent_dropout=recurr_dropout,return_sequences=True))\n",
    "        model.add(LSTM(unit, activation=activ,dropout_rate, recurrent_dropout=recurr_dropout))\n",
    "    elif(model_type == \"GRU\"):\n",
    "        if layer!=0:\n",
    "            for i in range(layer):\n",
    "                model.add(GRU(unit, activation=activ,dropout_rate, recurrent_dropout=recurr_dropout,return_sequences=True))\n",
    "        model.add(GRU(unit, activation=activ,dropout_rate, recurrent_dropout=recurr_dropout))\n",
    "    elif(model_type == \"BiLSTM\"):\n",
    "         if layer!=0:\n",
    "            for i in range(layer):\n",
    "                model.add(Bidirectional(LSTM(unit, activation=activ,dropout=dropout_rate, recurrent_dropout=recurr_dropout,return_sequences=True)))\n",
    "        model.add(Bidirectional(LSTM(unit, activation=activ,dropout=dropout_rate, recurrent_dropout=recurr_dropout)))\n",
    "    elif(model_type == \"BiGRU\"):\n",
    "        if layer!=0:\n",
    "            for i in range(layer):\n",
    "                model.add(Bidirectional(GRU(unit, activation=activ,dropout=dropout_rate, recurrent_dropout=,return_sequences=True)))\n",
    "        model.add(Bidirectional(GRU(unit, activation=activ,dropout=dropout_rate, recurrent_dropout=recurr_dropout)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(unit, activation=activ))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer_adam = Adam(learning_rate=TC.lr)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=optimizer_adam,\n",
    "                metrics=['acc'])\n",
    "    print(model.summary())\n",
    "    TC.model = model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_W.EPOCHS =3\n",
    "TC_W.BATCH_SIZE =16\n",
    "TC_W.lr = 0.001\n",
    "TC_W.tag = \"test optuna\"\n",
    "MODEL_TYPE = \"dense\"\n",
    "rnn_model = get_model(\"RNN\")\n",
    "rnn_model, hist = train_model(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = TC_W.model_evaluate(rnn_model,hist)\n",
    "TC_W.save_predictions(Y_pred,TC_W.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import trial\n",
    "import optuna.integration.TFKerasPruningCallback\n",
    "from tensorflow.python.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "def objective(trial):  # trial is object\n",
    "    # from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "    # from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    " \n",
    "    model = create_model(trial,TC_W,MODEL_TYPE)\n",
    "    # scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n",
    "    # lr = LearningRateScheduler(scheduler, verbose=0)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1,mode='min',restore_best_weights=True)\n",
    "    pruning = TFKerasPruningCallback(trial, \"val_loss\")\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-5,mode='min', verbose=verbose)\n",
    "    callbacks_list =[early_stopping,lr,pruning]\n",
    "        # Fit the model on the training data.\n",
    "        # The TFKerasPruningCallback checks for pruning condition every epoch.\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks_list,\n",
    "        epochs=TC_W.EPOCHS,\n",
    "        validation_data=0.1,\n",
    "        verbose=1,\n",
    "    )\n",
    "    #score = model.evaluate(TC_W.X_test, TC_W.Y_test, verbose=0)\n",
    "    #return score[1]\n",
    "    validation_loss = np.min(h.history['val_loss'])\n",
    "                 \n",
    "    return validation_loss\n",
    "    # f1_score = model_performence(model)\n",
    "    # return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 3\n",
    "neptune_callback = opt_utils.NeptuneMonitor(\n",
    "    # plots_update_freq=10,  # create/log plots every 10 trials\n",
    "    # log_plot_slice=False,  # do not create/log plot_slice\n",
    "    # log_plot_contour=False,  # do not create/log plot_contour\n",
    ")\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective,n_trials=N_TRIALS,callbacks=[neptune_callback])  # number of trials\n",
    " \n",
    "# Log Optuna charts and study object after the sweep is complete\n",
    "# optuna_utils.log_study_metadata(study,\n",
    "#                                 run,  \n",
    "#                                 log_plot_contour=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation\n",
    "def create_lstm_model(trial):\n",
    "\n",
    "    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n",
    "\n",
    "    lstm_layers = 4\n",
    "    lstm_units = np.zeros(lstm_layers, dtype=np.int)\n",
    "    lstm_units[0] = trial.suggest_int(\"lstm_units_L1\", 768, 1536)\n",
    "    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n",
    "    for i in range(lstm_layers-1):\n",
    "        lstm_units[i+1] = trial.suggest_int(\"lstm_units_L{}\".format(i+2), lstm_units[i]//2, lstm_units[i])\n",
    "        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n",
    "    dropout_rate = trial.suggest_float(\"lstm_dropout\", 0.0, 0.3)\n",
    "    lstm = Dropout(dropout_rate)(lstm)\n",
    "    dense_units = lstm_units[-1]\n",
    "    # try different activations\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\", \"swish\"])\n",
    "    lstm = Dense(dense_units, activation=activation)(lstm)\n",
    "    lstm = Dense(1)(lstm)\n",
    "\n",
    "    model = keras.Model(inputs=x0, outputs=lstm)\n",
    "    metrics = [\"mae\"]\n",
    "    model.compile(optimizer=\"adam\", loss=\"mae\", metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_seq,test_seq,Y_train,Y_test,tag):\n",
    "  \n",
    "  #define callbacks\n",
    "  \n",
    "  early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1,mode='min',restore_best_weights=True)\n",
    "  #callbacks_list = [early_stopping]\n",
    "\n",
    "  #model training\n",
    "  print(\"started training\")\n",
    "  hist = model.fit(train_seq, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[early_stopping],validation_split=0.1, shuffle=False, verbose=2)\n",
    "  train_loss, train_acc = model.evaluate(train_seq, Y_train,batch_size=BATCH_SIZE, verbose=1)\n",
    "  print(\"train loss - \",train_loss,\" train acc- \",train_acc)\n",
    "  test_loss, test_acc = model.evaluate(test_seq,Y_test,batch_size=BATCH_SIZE, verbose=1)\n",
    "  print(\"test loss - \",test_loss,\" test acc- \",test_acc)\n",
    "\n",
    "  # plot loss during training\n",
    "  #from matplotlib import pyplot\n",
    "  print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "  plt.subplot(211)\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel(\"Cross Entropy Loss\")\n",
    "  plt.plot(hist.history['loss'],lw=2.0, color='b', label='train')\n",
    "  plt.plot(hist.history['val_loss'],lw=2.0, color='r', label='validation')\n",
    "  plt.legend(loc='upper right')\n",
    "  # plot accuracy during training\n",
    "  plt.subplot(212)\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.plot(hist.history['accuracy'],lw=2.0, color='b', label='train')\n",
    "  plt.plot(hist.history['val_accuracy'],lw=2.0, color='r', label='validation')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.show()\n",
    "  #(model.predict(x) > 0.5).astype(\"int32\")\n",
    "  #np.where(y_pred > threshold, 1,0)\n",
    "  Y_pred =(model.predict(test_seq) > 0.5).astype(\"int32\")# model.predict_class(test_seq)\n",
    "  Y_pred = np.squeeze(Y_pred)\n",
    "  log_result(Y_test,Y_pred,\"fasttext\",\"RNN\",tag)\n",
    "  print(classification_report(Y_test,Y_pred))\n",
    "  #Y_pred = np.squeeze(Y_pred)\n",
    "  utills.confusion_Matrix(Y_test,Y_pred)\n",
    "  utills.PlotRocAuc(Y_test,Y_pred,'blue',\"RNN\")\n",
    "  return model,Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_result(Y_test,Y_pred,feature_name,model_name,tag):\n",
    "    print(\"========= Eperiment - \",tag,\" =========\")\n",
    "    neptune.create_experiment(tag,params=PARAMS)\n",
    "    neptune.append_tag(['DL experiment',feature_name,model_name,tag])\n",
    "    \n",
    "    log_class_metrics(Y_test, Y_pred)\n",
    "    log_confusion_matrix(Y_test, Y_pred)\n",
    "    log_classification_report(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras: optuna.integration.KerasPruningCallback\n",
    "TensorFlow optuna.integration.TensorFlowPruningHook\n",
    "tf.keras optuna.integration.TFKerasPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=2, reduction_factor=4, min_early_stopping_rate=1))\n",
    "study.optimize(objective_with_prune, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective, n_trials=100)\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_df = study.trials_dataframe()\n",
    "trials_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, 'optuna_searches/study.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "# #         lr = WarmupExponentialDecay(lr_base=1e-3, decay=1e-5, warmup_epochs=30)\n",
    "#         es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "    \n",
    "#         checkpoint_filepath = f\"folds{fold}.hdf5\"\n",
    "#         sv = keras.callbacks.ModelCheckpoint(\n",
    "#             checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "#             save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "#             options=None\n",
    "#         )\n",
    "\n",
    "#         model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n",
    "#         #model.save(f'Fold{fold+1} RNN Weights')\n",
    "#         test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log single String\n",
    "run['aux/text'] = 'text I keep track of, like query or tokenized word'\n",
    "\n",
    "# Log series of String to one log\n",
    "for epoch in range(epochs_nr):\n",
    "    token = str(...)\n",
    "    run['train/tokens'].log(token)\n",
    "## CSV file\n",
    "#run['test/preds'].upload('path/to/test_preds.csv')\n",
    "from neptune.new.types import File\n",
    "\n",
    "# Pandas DataFrame \n",
    "#run['data/sample'].upload(File.as_html(sample_df))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
